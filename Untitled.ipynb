{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import tflearn.variables as va\n",
    "\n",
    "# Loading MNIST dataset\n",
    "import tflearn.datasets.mnist as mnist\n",
    "trainX, trainY, testX, testY = mnist.load_data(one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import json\n",
    "import re\n",
    "# 两联体编码\n",
    "def dAAOneHot(fastafile):\n",
    "    daa=[x+y for x in text for y in text]\n",
    "    X = []\n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "        seq = str(seq_record.seq)\n",
    "        seq = re.sub('[XZUB]',\"\",seq)\n",
    "        t = np.zeros(400)\n",
    "        for j in range(400):\n",
    "            t[j] = seq.count(daa[j])\n",
    "        s = sum(t)\n",
    "        for j in range(400):\n",
    "            t[j] = t[j]/s\n",
    "        X.append(t)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='PQRYWTMNVELHSFCIKADG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对矩阵进行归一化\n",
    "def maxminnorm(array):\n",
    "    maxcols=array.max(axis=0)\n",
    "    mincols=array.min(axis=0)\n",
    "    data_shape = array.shape\n",
    "    data_rows = data_shape[0]\n",
    "    data_cols = data_shape[1]\n",
    "    t=np.empty((data_rows,data_cols))\n",
    "    for i in range(data_cols):\n",
    "        if maxcols[i] > mincols[i]:\n",
    "            t[:,i]=(array[:,i]-mincols[i])/(maxcols[i]-mincols[i])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载来自hmmer profil的数据\n",
    "# jsonfile 存储hmmer profil数据的json格式文件名\n",
    "# fastafile 序列的参照文件，fasta格式\n",
    "# numAA>0 表示从头取numAA个氨基酸的profil，尾不足补0; numAA<0 表示从尾往前取numAA个氨基酸的profil，头不足补0\n",
    "def load_hmm_prof(jsonfile, fastafile, numAA=50):\n",
    "    records = SeqIO.parse(fastafile, 'fasta')\n",
    "    seqID = [str(x.id) for x in records]\n",
    "    records.close()\n",
    "    \n",
    "    M = len(seqID)\n",
    "    N = abs(numAA) * 20\n",
    "    \n",
    "    X = np.ndarray((M,N))\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    fr = open(jsonfile,'r')\n",
    "    p = json.load(fr)\n",
    "    fr.close()\n",
    "    \n",
    "    for key in seqID:\n",
    "        ary = p[key]\n",
    "        tm = np.array(ary).reshape([-1,20])\n",
    "        tm = tm[1:,:]\n",
    "        c = len(ary)-20\n",
    "        if numAA > 0:\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k][:c] = tm.reshape(c)\n",
    "                X[k][c:] = 0\n",
    "            elif c == N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[:numAA,:]\n",
    "                t = maxminnorm(t)# 归一化\n",
    "                X[k] = t.reshape(N)\n",
    "        else:# numAA < 0\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)\n",
    "                X[k][-c:] = tm.reshape(c)\n",
    "                X[k][:-c] = 0\n",
    "            elif c==N:\n",
    "                tm = maxminnorm(tm)\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[numAA:,:]\n",
    "                t = maxminnorm(t)\n",
    "                X[k] = t.reshape(N)\n",
    "\n",
    "        k += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = ['antiB', 'antiC', 'antiF', 'antiH', 'antiP', 'antiV',\n",
    "            'antiA', 'antiD', 'antiE', 'antiI', 'antiO', 'antiS', 'antiT', 'antiW', 'surface', 'taxis']\n",
    "#include_T = ['antiB', 'antiC', 'antiF', 'antiH', 'antiP', 'antiV']\n",
    "#exclude_T = ['antiA', 'antiD', 'anitE', 'antiI', 'antiO', 'antiS', 'antiT', 'antiW', 'surface', 'taxis' ]\n",
    "targetFile = './data/benchmark/amps_60_Targets.json'\n",
    "seqFile = './data/benchmark/amps_60_Sequence.json'\n",
    "include_fastafile = './data/benchmark/include_amps_60.fasta'\n",
    "exclude_fastafile = './data/benchmark/exclude_amps_60.fasta'\n",
    "\n",
    "seq_recorders = SeqIO.parse(include_fastafile, 'fasta')\n",
    "seq_ids = [str(r.id) for r in seq_recorders]\n",
    "seq_recorders.close()\n",
    "\n",
    "M = len(seq_ids)\n",
    "\n",
    "# 构建标签数据。共分6类，是include_T中包含的标签。\n",
    "# 构建一个6-by-6的矩阵，\n",
    "ft = open(targetFile,'r')\n",
    "targets = json.load(ft)\n",
    "ft.close()\n",
    "\n",
    "y = np.zeros([M,6])\n",
    "j = 0\n",
    "for key in seq_ids:\n",
    "    keys = targets[key]\n",
    "    for k in keys:\n",
    "        i = families.index(k)\n",
    "        if i < 6:\n",
    "            y[j][i] = 1\n",
    "    j+=1\n",
    "\n",
    "X1 = load_hmm_prof('./data/benchmark/includeAMPs_hmm_profil.json',include_fastafile,20)\n",
    "X2 = load_hmm_prof('./data/benchmark/includeAMPs_hmm_profil.json',include_fastafile,-20)\n",
    "X3 = np.array(dAAOneHot(include_fastafile))\n",
    "\n",
    "X = np.ndarray([X1.shape[0],20,20,3])\n",
    "X[:,:,:,0] = X1.reshape([-1,20,20])\n",
    "X[:,:,:,1] = X2.reshape([-1,20,20])\n",
    "X[:,:,:,2] = X3.reshape([-1,20,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflearn.data_utils import shuffle\n",
    "X,y = shuffle(X,y)\n",
    "X_train,X_test=X[:1100],X[1100:]\n",
    "y_train,y_test=y[:1100],y[1100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m3.18247\u001b[0m\u001b[0m | time: 1.783s\n",
      "| Optimizer | epoch: 001 | loss: 3.18247 - Summaries/acc: 0.8709 -- iter: 1024/1100\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m3.48769\u001b[0m\u001b[0m | time: 2.947s\n",
      "| Optimizer | epoch: 001 | loss: 3.48769 - Summaries/acc: 0.4388 | val_loss: 4.83203 - val_acc: 0.8921 -- iter: 1100/1100\n",
      "--\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'pred_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-91774b0f5d99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# >> tensorboard /tmp/tflearn_logs/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m# Navigate with your web browser to http://0.0.0.0:6006/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mpred_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'pred_label'"
     ]
    }
   ],
   "source": [
    "# Define a dnn using Tensorflow\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Model variables\n",
    "    XX = tf.placeholder(\"float\", [None,20,20,3])\n",
    "    YY = tf.placeholder(\"float\", [None, 6])\n",
    "\n",
    "    # Using TFLearn wrappers for network building\n",
    "    #net = tf.reshape(XX, [-1, 28, 28, 1])\n",
    "    net = tflearn.conv_2d(XX, 32, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    net = tflearn.local_response_normalization(net)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.conv_2d(net, 64, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    net = tflearn.local_response_normalization(net)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 128, activation='tanh')\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    #net = tflearn.fully_connected(net, 256, activation='tanh')\n",
    "    #net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 6, activation='linear')\n",
    "    \n",
    "    with tf.name_scope('Summaries'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=net,labels=YY))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(YY, 1)), tf.float32),\n",
    "            name=\"acc\")\n",
    "\n",
    "    # construct two varaibles to add as additional \"valiation monitors\"\n",
    "    # these varaibles are evaluated each time validation happens (eg at a snapshot)\n",
    "    # and the results are summarized and output to the tensorboard events file,\n",
    "    # together with the accuracy and loss plots.\n",
    "    #\n",
    "    # Here, we generate a dummy variable given by the sum over the current\n",
    "    # network tensor, and a constant variable.  In practice, the validation\n",
    "    # monitor may present useful information, like confusion matrix\n",
    "    # entries, or an AUC metric.\n",
    "    #with tf.name_scope('CustomMonitor'):\n",
    "    #    test_var = tf.reduce_sum(tf.cast(net, tf.float32), name=\"test_var\")\n",
    "    #    test_const = tf.constant(32.0, name=\"custom_constant\")\n",
    "        # Define a train op\n",
    "    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n",
    "                           # validation_monitors=[test_var, test_const],\n",
    "                            metric=accuracy, batch_size=128)\n",
    "\n",
    "    # Tensorboard logs stored in /tmp/tflearn_logs/. Using verbose level 2.\n",
    "    trainer = tflearn.Trainer(train_ops=trainop,\n",
    "                              tensorboard_dir='/tmp/tflearn_logs/',\n",
    "                              tensorboard_verbose=2)\n",
    "    # Training for 10 epochs.\n",
    "    trainer.fit({XX: X_train, YY: y_train}, val_feed_dicts={XX: X_test, YY: y_test},\n",
    "                n_epoch=1, show_metric=True, run_id='Summaries_example')\n",
    "\n",
    "    # Run the following command to start tensorboard:\n",
    "    # >> tensorboard /tmp/tflearn_logs/\n",
    "# Navigate with your web browser to http://0.0.0.0:6006/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 7EP0AW\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name acc_0/ (raw) is illegal; using acc_0/__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name acc_0/ (raw) is illegal; using acc_0/__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Training samples: 55000\n",
      "Validation samples: 10000\n",
      "--\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tag: acc:0 cannot be found in summaries list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d71fe0d33001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Training for 10 epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     trainer.fit({X: trainX, Y: trainY}, val_feed_dicts={X: testX, Y: testY},\n\u001b[0;32m---> 39\u001b[0;31m n_epoch=1, show_metric=True)\n\u001b[0m",
      "\u001b[0;32m~/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                                        \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                             \u001b[0;31m# Update training state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0msname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_summ_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m             self.acc_value = summaries.get_value_from_summary_string(\n\u001b[0;32m--> 829\u001b[0;31m                 sname, train_summ_str)\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tflearn/summaries.py\u001b[0m in \u001b[0;36mget_value_from_summary_string\u001b[0;34m(tag, summary_str)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tag: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" cannot be found in summaries list.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tag: acc:0 cannot be found in summaries list."
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Model variables\n",
    "    X = tf.placeholder(\"float\", [None, 784])\n",
    "    Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "    W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "    W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "    W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "    b1 = tf.Variable(tf.random_normal([256]))\n",
    "    b2 = tf.Variable(tf.random_normal([256]))\n",
    "    b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "    # Multilayer perceptron\n",
    "    def dnn(x):\n",
    "        x = tf.nn.tanh(tf.add(tf.matmul(x, W1), b1))\n",
    "        x = tf.nn.tanh(tf.add(tf.matmul(x, W2), b2))\n",
    "        x = tf.add(tf.matmul(x, W3), b3)\n",
    "        return x\n",
    "\n",
    "    net = dnn(X)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=Y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1)), tf.float32),\n",
    "        name='acc')\n",
    "\n",
    "    # Using TFLearn Trainer\n",
    "    # Define a training op (op for backprop, only need 1 in this model)\n",
    "    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n",
    "                              metric=accuracy, batch_size=128)\n",
    "\n",
    "    # Create Trainer, providing all training ops. Tensorboard logs stored\n",
    "    # in /tmp/tflearn_logs/. It is possible to change verbose level for more\n",
    "    # details logs about gradients, variables etc...\n",
    "    trainer = tflearn.Trainer(train_ops=trainop, tensorboard_verbose=0)\n",
    "    # Training for 10 epochs.\n",
    "    trainer.fit({X: trainX, Y: trainY}, val_feed_dicts={X: testX, Y: testY},\n",
    "n_epoch=1, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69314718 0.31326169 1.31326169]\n",
      " [0.31326169 0.31326169 0.69314718]\n",
      " [0.69314718 0.69314718 0.31326169]]\n",
      ".............\n",
      "0.8888889\n",
      "[[1. 1. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    " \n",
    "labels1=np.array([[0.,1.,0.],[1.,1.,0.],[0.,0.,1.]])#不一定只属于一个类别\n",
    "logits1=np.array([[0,1,1.],[1,1,0.],[0.,0.,1]])\n",
    "y_pred1=sigmoid(logits1)\n",
    "prob_error11=-labels1*np.log(y_pred1)-(1-labels1)*np.log(1-y_pred1)\n",
    "print(prob_error11)\n",
    " \n",
    "print(\".............\")\n",
    "with tf.Session() as sess:\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels1,logits=logits1)\n",
    "    loss = tf.reduce_sum(loss, axis=1)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    acc1=tf.cast(tf.equal(labels1,logits1),tf.float32)\n",
    "    acc2=tf.reduce_mean(acc1)\n",
    "    print(sess.run(acc2))\n",
    "    print(sess.run(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
