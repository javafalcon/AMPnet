{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蛋白质编码\n",
    "\n",
    "读取AMPs和notAMPs序列，转化为2个通道的特征向量表示。其中通道1的数据来自hmmer的profile；通道2的数据来自氨基酸Onehot编码;通道3来自AA的物化性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#jsonfiles = ['./data/benchmark/AMPs_50_hmm_profil.json','./data/benchmark/notAMPs_50_hmm_profil.json']\n",
    "#fastafiles=['./data/benchmark/wpAMPs.fasta','./data/benchmark/wpnotAMPs.fasta']\n",
    "#files=['./data/benchmark/AMPs_50.fasta','./data/benchmark/notAMPs_50.fasta']\n",
    "text='PQRYWTMNVELHSFCIKADG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对矩阵进行归一化\n",
    "def maxminnorm(array):\n",
    "    maxcols=array.max(axis=0)\n",
    "    mincols=array.min(axis=0)\n",
    "    data_shape = array.shape\n",
    "    data_rows = data_shape[0]\n",
    "    data_cols = data_shape[1]\n",
    "    t=np.empty((data_rows,data_cols))\n",
    "    for i in range(data_cols):\n",
    "        if maxcols[i] > mincols[i]:\n",
    "            t[:,i]=(array[:,i]-mincols[i])/(maxcols[i]-mincols[i])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_hmm_prof: 读入序列的hmmer profil 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载来自hmmer profil的数据\n",
    "# jsonfile 存储hmmer profil数据的json格式文件名\n",
    "# fastafile 序列的参照文件，fasta格式\n",
    "# numAA>0 表示从头取numAA个氨基酸的profil，尾不足补0; numAA<0 表示从尾往前取numAA个氨基酸的profil，头不足补0\n",
    "def load_hmm_prof(jsonfile, fastafile, numAA=50):\n",
    "    records = SeqIO.parse(fastafile, 'fasta')\n",
    "    seqID = [str(x.id) for x in records]\n",
    "    records.close()\n",
    "    \n",
    "    M = len(seqID)\n",
    "    N = abs(numAA) * 20\n",
    "    \n",
    "    X = np.ndarray((M,N))\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    fr = open(jsonfile,'r')\n",
    "    p = json.load(fr)\n",
    "    fr.close()\n",
    "    \n",
    "    for key in seqID:\n",
    "        ary = p[key]\n",
    "        tm = np.array(ary).reshape([-1,20])\n",
    "        tm = tm[1:,:]\n",
    "        c = len(ary)-20\n",
    "        if numAA > 0:\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k][:c] = tm.reshape(c)\n",
    "                X[k][c:] = 0\n",
    "            elif c == N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[:numAA,:]\n",
    "                t = maxminnorm(t)# 归一化\n",
    "                X[k] = t.reshape(N)\n",
    "        else:# numAA < 0\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)\n",
    "                X[k][-c:] = tm.reshape(c)\n",
    "                X[k][:-c] = 0\n",
    "            elif c==N:\n",
    "                tm = maxminnorm(tm)\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[numAA:,:]\n",
    "                t = maxminnorm(t)\n",
    "                X[k] = t.reshape(N)\n",
    "\n",
    "        k += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 氨基酸OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numAA=0 蛋白质序列全肽链； numAA>0 从头截止到第numAA个氨基酸； numAA<0 从尾部往前numAA个氨基酸\n",
    "# 返回一个列表，列表的每一行表示一个蛋白质序列的OneHot编码\n",
    "def AAOneHot(fastafile, numAA=50):\n",
    "    X = []\n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "            seq = str(seq_record.seq)\n",
    "            seq = re.sub('[XZUB]',\"\",seq)\n",
    "            c = len(seq)\n",
    "            m = np.zeros([c,20])\n",
    "            for i in range(c):\n",
    "                j = text.index(seq[i])\n",
    "                m[i][j] = 1.0\n",
    "            m = m.reshape([1,-1])\n",
    "            X.append(m)\n",
    "            \n",
    "    if numAA == 0:\n",
    "        return X\n",
    "    elif numAA> 0:\n",
    "        L = numAA*20\n",
    "        A = []\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[:c] = m\n",
    "                t[c:] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[:L]\n",
    "            A.append(t)\n",
    "        return A\n",
    "    else: # numAA < 0\n",
    "        L = abs(numAA)*20\n",
    "        A = []\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[-c:] = m\n",
    "                t[:-c] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[-L:]\n",
    "            A.append(t)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dAAOneHot(): 氨基酸两联体编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 两联体编码\n",
    "def dAAOneHot(fastafile):\n",
    "    daa=[x+y for x in text for y in text]\n",
    "    X = []\n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "        seq = str(seq_record.seq)\n",
    "        seq = re.sub('[XZUB]',\"\",seq)\n",
    "        t = np.zeros(400)\n",
    "        for j in range(400):\n",
    "            t[j] = seq.count(daa[j])\n",
    "        s = sum(t)\n",
    "        for j in range(400):\n",
    "            t[j] = t[j]/s\n",
    "        X.append(t)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 氨基酸物化属性OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numAA=0 蛋白质序列全肽链； numAA>0 从头截止到第numAA个氨基酸； numAA<0 从尾部往前numAA个氨基酸\n",
    "# 返回一个列表，列表的每一行表示一个蛋白质序列的物化属性的OneHot编码\n",
    "def AAPhyChemOneHot(fastafile, numAA):\n",
    "    phychemDict={}\n",
    "    phychemDict[\"alcohol\"]=(\"S\",\"T\")# 有乙醇基\n",
    "    phychemDict[\"aliphatic\"]=(\"I\",\"L\",\"V\")# 脂肪族\n",
    "    phychemDict[\"aromatic\"]=(\"F\",\"H\",\"W\",\"Y\")# 芳香族\n",
    "    phychemDict[\"charged\"]=(\"D\",\"E\",\"H\",\"K\",\"R\")# 带电性\n",
    "    phychemDict[\"positive\"]=(\"K\",\"H\",\"R\")# 带正电\n",
    "    phychemDict[\"negative\"]=(\"D\",\"E\")# 带负电\n",
    "    phychemDict[\"polar\"]=(\"A\",\"L\",\"I\",\"P\",\"F\",\"W\",\"M\")# 非极性\n",
    "    phychemDict[\"small\"]=(\"A\",\"C\",\"D\",\"G\",\"N\",\"P\",\"S\",\"T\",\"V\")# 小分子\n",
    "    phychemDict[\"turnlike\"]=(\"A\",\"C\",\"D\",\"E\",\"G\",\"H\",\"K\",\"N\",\"Q\",\"R\",\"S\",\"T\")\n",
    "    phychemDict[\"hydrophobic\"]=(\"A\",\"F\",\"I\",\"L\",\"M\",\"P\",\"V\",\"W\",\"Y\")# 疏水\n",
    "    phychemDict[\"asa\"]=(\"A\",\"N\",\"D\",\"C\",\"P\",\"S\",\"T\",\"G\",\"V\")# 可溶解表面积低于平均值\n",
    "    phychemDict[\"pr\"]=(\"F\",\"Y\",\"W\")# 在紫外区有光吸收能力\n",
    "       \n",
    "    X = []\n",
    "    keys = phychemDict.keys()\n",
    "    lskey = list(keys)    \n",
    "    N = len(keys)\n",
    "    \n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "        seq = str(seq_record.seq)\n",
    "        seq = re.sub('[XZUB]',\"\",seq)\n",
    "        c = len(seq)\n",
    "        m = np.zeros((len(seq),N))\n",
    "        \n",
    "        for i in range(c):\n",
    "            for j in range(N):\n",
    "                key = lskey[j]\n",
    "                val = phychemDict[key]\n",
    "                if seq[i] in val:\n",
    "                    m[i][j] = 1\n",
    " \n",
    "        m = m.reshape((1,-1))\n",
    "        X.append(m)\n",
    "        \n",
    "    if numAA == 0:\n",
    "        return X\n",
    "    elif numAA > 0:# 只截取蛋白质序列前numAA个氨基酸，不足在尾部补0\n",
    "        A = []\n",
    "        L = numAA*20\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[:c] = m\n",
    "                t[c:] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[:L] \n",
    "            A.append(t)\n",
    "        return A\n",
    "    else: # numAA<0 截取蛋白质序列后numAA个氨基酸，不足在头部补0\n",
    "        A = []\n",
    "        L = abs(numAA)*20\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[-c:] = m\n",
    "                t[:-c] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[-L:]\n",
    "            A.aapend(t)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建卷积网络进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入tensorflow的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Alex网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alexnet(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    # Building 'AlexNet'\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels])\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,10,10,96]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,5,5,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,3,3,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "    network = regression(network, optimizer='momentum',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Cifar网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cifarnet(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "    \n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "    \n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = dropout(network, 0.75)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 512, activation='sigmoid')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 512, activation='sigmoid')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, num_classes, activation='sigmoid')\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    return network\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建vgg网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building 'VGG Network'\n",
    "def create_vggnet(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels])\n",
    "\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,25,10,64]\n",
    "\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,13,5,128]\n",
    "\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,7,3,256]\n",
    "    \n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    \n",
    "    network = fully_connected(network, 2048, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "\n",
    "    network = regression(network, optimizer='rmsprop',\n",
    "                         loss='softmax_categorical_crossentropy',\n",
    "                         learning_rate=0.0001)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其它网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net1(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels])\n",
    "    network = fully_connected(network,128,activation='sigmoid',\n",
    "                             regularizer='L2',\n",
    "                             weight_decay=0.001)\n",
    "    network = fully_connected(network,256,activation='sigmoid',\n",
    "                             regularizer='L2',\n",
    "                             weight_decay=0.001)\n",
    "    #network = fully_connected(network,360,activation='relu',\n",
    "    #                         regularizer='L2',\n",
    "    #                         weight_decay=0.001)\n",
    "    network = dropout(network,0.8)\n",
    "    network = fully_connected(network, num_classes, activation=\"sigmoid\")\n",
    "    network = regression(network, optimizer='adam',\n",
    "                        loss=\"binary_crossentropy\")\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "def cnn_1d(n_features,num_classes):\n",
    "    network = input_data(shape=[None, n_features], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=10000, output_dim=128)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义交叉验证函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jack knife测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 返回预测的结果，预测的结果为各个类的概率\n",
    "def jackknife_test(X, y, imgrow=20, imgcol=20, num_classes=2,channels=3,n_epoch=100):\n",
    "    M = X.shape[0]\n",
    "    y_pred = np.zeros([M,2])\n",
    "    loo = LeaveOneOut()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        print(\"\\r In predicting {}\".format(test_index))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tf.reset_default_graph()\n",
    "        net = create_alexnet(imgrow, imgcol, num_classes,channels)\n",
    "        model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "        model.fit(X_train, y_train, n_epoch, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32, \n",
    "              run_id='AMP_cnn')\n",
    "        pred_prob[test_index] = model.predict(X_test)\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold折叠验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 返回预测的结果，预测的结果为各个类的概率\n",
    "def cross_validate(X,y,n_splits=3,imgrow=20, imgcol=20, num_classes=2, channels=3, n_epoch=100):\n",
    "    M = X.shape[0]\n",
    "    pred_prob = np.zeros([M,2])\n",
    "    kf = KFold(n_splits)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tf.reset_default_graph()\n",
    "        net = create_alexnet(imgrow, imgcol, num_classes, channels)\n",
    "        model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "        model.fit(X_train, y_train, n_epoch, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32)\n",
    "        pred_prob[test_index] = model.predict(X_test)\n",
    "        \n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算预测性能（指标：ACC、AUC和MCC）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算准确率acc和可接受曲线下面积AUC\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "def metric(y, predprob):\n",
    "    d1 = len(y)\n",
    "    y_pred = np.zeros((d1,2))\n",
    "    for i in range(d1):\n",
    "        if predprob[i][0] > predprob[i][1]:\n",
    "            y_pred[i][0] = 1\n",
    "        else:\n",
    "            y_pred[i][1] = 1\n",
    "            \n",
    "    accuracy=accuracy_score(y[:,0],y_pred[:,0])\n",
    "    print(\"accuracy={}\".format(accuracy))\n",
    "    fpr,tpr,thresholds=roc_curve(y[:,0],predprob[:,0],pos_label=1)\n",
    "    print(\"AUC={}\".format(auc(fpr,tpr)))\n",
    "    mcc = matthews_corrcoef(y[:,0],y_pred[:,0])\n",
    "    print(\"mcc={}\".format(mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 伪氨基酸成分数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载arff文件，读入879个AMps和2405个非AMPs样本的特征\n",
    "from scipy.io import arff\n",
    "data,meta = arff.loadarff('./data/benchmark/wp_amp_notamp_30features.arff')\n",
    "n = len(data)\n",
    "X = np.ndarray((n,30))\n",
    "Y = np.zeros(n)\n",
    "for i in range(n):\n",
    "    d = data[i]\n",
    "    for j in range(30):\n",
    "        X[i][j] = float(d[j])\n",
    "    Y[i] = abs(int(d[-1])-1)\n",
    "y = to_categorical(Y,2)\n",
    "X1,y = shuffle(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predprob = cross_validate(X1,y,n_splits=5,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predprob = jackknife_test(X1,y,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一层预测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用iAMP-2L的数据训练及预测\n",
    "<p/>\n",
    "<font size=4>数据由王普构建，包含879个AMP和2405个非抗菌他</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonfiles_wp = ['./data/benchmark/wpAMPs_hmm_profil.json','./data/benchmark/wpnotAMPs_hmm_profil.json']\n",
    "files_wp=['./data/benchmark/wpAMPs.fasta','./data/benchmark/wpnotAMPs.fasta']\n",
    "N = 20\n",
    "X1_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],N)\n",
    "X1_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],N)\n",
    "X1 = np.vstack((X1_1, X1_2)).reshape([-1,N,20])\n",
    "\n",
    "X2_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],-N)\n",
    "X2_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],-N)\n",
    "X2 = np.vstack((X2_1, X2_2)).reshape([-1,N,20])\n",
    "\n",
    "m1 = X1_1.shape[0]\n",
    "m2 = X1_2.shape[0]\n",
    "\n",
    "if N > 20:\n",
    "    t = np.array(dAAOneHot(files_wp[0]))\n",
    "    X3_1 = np.zeros([m1,N,20])\n",
    "    X3_1[:,:20,:] = t.reshape([-1,20,20])\n",
    "    t = np.array(dAAOneHot(files_wp[1]))\n",
    "    X3_2 = np.zeros([m2,N,20])\n",
    "    X3_2[:,:20,:] = t.reshape([-1,20,20])\n",
    "    X3 = np.vstack((X3_1, X3_2))\n",
    "else:\n",
    "    X3_1 = np.array(dAAOneHot(files_wp[0]))\n",
    "    X3_2 = np.array(dAAOneHot(files_wp[1]))\n",
    "    X3 = np.vstack((X3_1, X3_2)).reshape([-1,N,20])\n",
    "    \n",
    "X = np.ndarray([m1+m2,N,20,3])\n",
    "X[:,:,:,0] = X1\n",
    "X[:,:,:,1] = X2\n",
    "X[:,:,:,2] = X3\n",
    "\n",
    "y = np.zeros(m1+m2)\n",
    "y[m1:] = 1\n",
    "y = to_categorical(y,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jack-knife 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = jackknife_test(X,y,20,20,2,3,30)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = cross_validate(X,y,n_splits=10,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><strong>预测结果：</strong></font>\n",
    " - n_splits=10,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30,alexnet:<br>\n",
    " accuracy=0.8815, AUC=0.9325,mcc=0.6993<br>\n",
    " - n_splits=10,imgrow=50,imgcol=20,num_classes=2,channels=3,n_epoch=30:<br>\n",
    " accuracy=0.8846, AUC=0.9428, mcc=0.7079<br> \n",
    " - n_splits=100,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30,alexnet:<br>\n",
    " accuracy=0.889768574908648, AUC=0.9450779211871362,mcc=0.7182219182285516<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用自己构建的数据集训练和测试\n",
    "<p></p>\n",
    "<font size=4>包含800个AMP和800个非AMP</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 构建训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonfiles_wp = ['./data/benchmark/AMPs_50_hmm_profil.json','./data/benchmark/notAMPs_50_hmm_profil.json']\n",
    "files_wp=['./data/benchmark/AMPs_50.fasta','./data/benchmark/notAMPs_50.fasta']\n",
    "N = 20\n",
    "X1_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],N)\n",
    "X1_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],N)\n",
    "X1 = np.vstack((X1_1, X1_2)).reshape([-1,N,20])\n",
    "\n",
    "X2_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],-N)\n",
    "X2_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],-N)\n",
    "X2 = np.vstack((X2_1, X2_2)).reshape([-1,N,20])\n",
    "\n",
    "X3_1 = np.array(dAAOneHot(files_wp[0]))\n",
    "X3_2 = np.array(dAAOneHot(files_wp[1]))\n",
    "X3 = np.vstack((X3_1, X3_2)).reshape([-1,20,20])\n",
    "\n",
    "m1 = X1_1.shape[0]\n",
    "m2 = X1_2.shape[0]\n",
    "\n",
    "X = np.ndarray([m1+m2,20,20,3])\n",
    "X[:,:,:,0] = X1\n",
    "X[:,:,:,1] = X2\n",
    "X[:,:,:,2] = X3\n",
    "\n",
    "y = np.zeros(m1+m2)\n",
    "y[m1:] = 1\n",
    "y = to_categorical(y,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jack-knife测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = jackknife_test(X,y,2,3,35)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二层预测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入多标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "families = ['antiB', 'antiC', 'antiF', 'antiH', 'antiP', 'antiV',\n",
    "            'antiA', 'antiD', 'antiE', 'antiI', 'antiO', 'antiS', 'antiT', 'antiW', 'surface', 'taxis']\n",
    "#include_T = ['antiB', 'antiC', 'antiF', 'antiH', 'antiP', 'antiV']\n",
    "#exclude_T = ['antiA', 'antiD', 'anitE', 'antiI', 'antiO', 'antiS', 'antiT', 'antiW', 'surface', 'taxis' ]\n",
    "targetFile = './data/benchmark/amps_60_Targets.json'\n",
    "seqFile = './data/benchmark/amps_60_Sequence.json'\n",
    "include_fastafile = './data/benchmark/include_amps_60.fasta'\n",
    "exclude_fastafile = './data/benchmark/exclude_amps_60.fasta'\n",
    "\n",
    "seq_recorders = SeqIO.parse(include_fastafile, 'fasta')\n",
    "seq_ids = [str(r.id) for r in seq_recorders]\n",
    "seq_recorders.close()\n",
    "\n",
    "M = len(seq_ids)\n",
    "\n",
    "# 构建标签数据。共分6类，是include_T中包含的标签。\n",
    "# 构建一个6-by-6的矩阵，\n",
    "ft = open(targetFile,'r')\n",
    "targets = json.load(ft)\n",
    "ft.close()\n",
    "\n",
    "y = np.zeros([M,6])\n",
    "j = 0\n",
    "for key in seq_ids:\n",
    "    keys = targets[key]\n",
    "    for k in keys:\n",
    "        i = families.index(k)\n",
    "        if i < 6:\n",
    "            y[j][i] = 1\n",
    "    j+=1\n",
    "\n",
    "X1 = load_hmm_prof('./data/benchmark/includeAMPs_hmm_profil.json',include_fastafile,20)\n",
    "X2 = load_hmm_prof('./data/benchmark/includeAMPs_hmm_profil.json',include_fastafile,-20)\n",
    "X3 = np.array(dAAOneHot(include_fastafile))\n",
    "\n",
    "X = np.ndarray([X1.shape[0],20,20,3])\n",
    "X[:,:,:,0] = X1.reshape([-1,20,20])\n",
    "X[:,:,:,1] = X2.reshape([-1,20,20])\n",
    "X[:,:,:,2] = X3.reshape([-1,20,20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jack-knife测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = cross_validate(X,y,n_splits=10,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "X_train,X_test=X[:1100],X[1100:]\n",
    "y_train,y_test=y[:1100],y[1100:]\n",
    "tf.reset_default_graph()\n",
    "net = net1(20, 20, 6, 3)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, 20, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32)\n",
    "pred_prob = model.predict(X_test)\n",
    "pred_label = model.predict_label(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仅hmmer profil一个通道数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1,y = load_hmm_prof()\n",
    "X1 = X1.reshape([-1,50,20,1])\n",
    "y = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    y[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    y[i][1] = 1\n",
    "# 交叉验证\n",
    "X1,y = shuffle(X1,y)\n",
    "predprob = cross_validate(X1,y,n_splits=5,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两个通道数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "X,y = getTwoChannelsArray()\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1\n",
    "# 交叉验证\n",
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=2,n_epoch=50)\n",
    "metric(y, predprob) # print: 0.81375, 0.883389\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> <b>预测结果</b></font><br>\n",
    "5-fold, cifanet,30 epochs, 879amps+2405 not amps, acc=0.8562, auc=0.9045, mcc=0.6470<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三个通道数据训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = getThreeChannelsArray()\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1\n",
    "# 交叉验证\n",
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y, predprob)# accuracy=0.820625 AUC=0.9052437499999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_prob = np.zeros(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test=X[:1280],X[1280:]\n",
    "y_train,y_test=y[:1280],y[1280:]\n",
    "tf.reset_default_graph()\n",
    "net = create_vggnet(2,3)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, n_epoch=30, shuffle=True, \n",
    "      show_metric=True, batch_size=32)\n",
    "predval = model.predict(X_test)\n",
    "metric(y_test, predval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predval = model.predict(X_test)\n",
    "metric(y_test, predval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>预测结果</b></font><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两联体-前20hmmrof-后20hmmprof,三个通道数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建数据集\n",
    "X1,y1 = load_hmm_prof(20,0)\n",
    "X2,y2 = load_hmm_prof(20,-1)\n",
    "X3,y3 = dAAOneHot()\n",
    "X=np.ndarray([M,20,20,3])\n",
    "X11 = X1.reshape([M,20,20])\n",
    "X21 = X2.reshape([M,20,20])\n",
    "X31 = X3.reshape([M,20,20])\n",
    "X[:,:,:,0]=X11\n",
    "X[:,:,:,1]=X21\n",
    "X[:,:,:,2]=X31\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=3,n_epoch=40)\n",
    "metric(y, predprob)# 王普的数据，用alexnet, acc=0.90956 auc=0.957706  mcc=0.765609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仅测试代码用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "net = create_cifarnet(2,3)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, 1, shuffle=True, \n",
    "      validation_set=(X_test,y_test),\n",
    "      show_metric=True, batch_size=32)\n",
    "pred_prob= model.predict(X_test)\n",
    "metric(y_test, pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    print(\"NO. {} function has {} samples, {}%\".format(i,sum(y[:,i]), 100*sum(y[:,i])/1378))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.zeros([10,5,4])\n",
    "x1=np.ones([10,2,4])\n",
    "x[:,:2,:]=x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.104px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
