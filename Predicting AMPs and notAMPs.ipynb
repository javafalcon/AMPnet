{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 蛋白质编码\n",
    "\n",
    "读取AMPs和notAMPs序列，转化为2个通道的特征向量表示。其中通道1的数据来自hmmer的profile；通道2的数据来自氨基酸Onehot编码;通道3来自AA的物化性质"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import json\n",
    "M1 = 879\n",
    "M2 = 2405\n",
    "M = M1+M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对矩阵进行归一化\n",
    "def maxminnorm(array):\n",
    "    maxcols=array.max(axis=0)\n",
    "    mincols=array.min(axis=0)\n",
    "    data_shape = array.shape\n",
    "    data_rows = data_shape[0]\n",
    "    data_cols = data_shape[1]\n",
    "    t=np.empty((data_rows,data_cols))\n",
    "    for i in range(data_cols):\n",
    "        t[:,i]=(array[:,i]-mincols[i])/(maxcols[i]-mincols[i])\n",
    "    return t\n",
    "\n",
    "# 加载来自hmmer profil的数据\n",
    "def load_hmm_prof():\n",
    "    #files = ['e:/repoes/ampnet/data/benchmark/AMPs_50_hmm_profil.json',\n",
    "    #     'e:/repoes/ampnet/data/benchmark/notAMPs_50_hmm_profil.json']\n",
    "    files = ['e:/repoes/ampnet/data/benchmark/wpAMPs_hmm_profil.json',\n",
    "        'e:/repoes/ampnet/data/benchmark/wpnotAMPs_hmm_profil.json']\n",
    "    N = 1000\n",
    "    X = np.ndarray((M,N))\n",
    "    y = np.ones(M)\n",
    "    y[M1:] = 0\n",
    "    k = 0\n",
    "    for f in files:\n",
    "        fr = open(f,'r')\n",
    "        p = json.load(fr)\n",
    "        for key in p.keys():\n",
    "            ary = p[key]\n",
    "            tm = np.array(ary).reshape([-1,20])\n",
    "            c = len(ary)\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k][:c] = tm.reshape(c)\n",
    "                X[k][c:] = 0\n",
    "            elif c == N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[:50,:]\n",
    "                t = maxminnorm(t)# 归一化\n",
    "                X[k] = t.reshape(N)\n",
    "            k += 1\n",
    "        fr.close()\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def AAOneHot():\n",
    "    #files=[r'E:\\Repoes\\AMPnet\\data\\benchmark\\AMPs_50.fasta',r'E:\\Repoes\\AMPnet\\data\\benchmark\\notAMPs_50.fasta']\n",
    "    files=[r'E:\\Repoes\\AMPnet\\data\\benchmark\\wpAMPs.fasta',r'E:\\Repoes\\AMPnet\\data\\benchmark\\wpnotAMPs.fasta']\n",
    "    text='PQRYWTMNVELHSFCIKADG'\n",
    "    N = 1000\n",
    "    X = np.ndarray((M,N))\n",
    "    y = np.ones(M)\n",
    "    y[M1:] = 0\n",
    "    k = 0\n",
    "    for file in files:\n",
    "        for seq_record in SeqIO.parse(file,'fasta'):\n",
    "            seq = str(seq_record.seq)\n",
    "            seq = re.sub('[XZUB]',\"\",seq)\n",
    "            #print(\"\\r{}\".format(seq),end=\"\")\n",
    "            c = len(seq)\n",
    "            m = np.zeros((len(seq),20))\n",
    "            for i in range(c):\n",
    "                j = text.index(seq[i])\n",
    "                m[i][j] = 1\n",
    "           \n",
    "            m = m.reshape((1,-1))\n",
    "            #print(\"in {},{}:{}\".format(file,seq_record.id,m.shape))\n",
    "            # 只截取蛋白质序列前50个aa，不足的补0\n",
    "            c = c*20\n",
    "            if c < N:\n",
    "                X[k][:c] = m[0]\n",
    "                X[k][c:] = 0\n",
    "            elif c == N:\n",
    "                X[k] = m[0]\n",
    "            else:\n",
    "                X[k] = m[0][:N] \n",
    "            k += 1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AAPhyChemOneHot():\n",
    "    #files=[r'E:\\Repoes\\AMPnet\\data\\benchmark\\AMPs_50.fasta',\n",
    "     #      r'E:\\Repoes\\AMPnet\\data\\benchmark\\notAMPs_50.fasta']\n",
    "    files=[r'E:\\Repoes\\AMPnet\\data\\benchmark\\wpAMPs.fasta',r'E:\\Repoes\\AMPnet\\data\\benchmark\\wpnotAMPs.fasta']\n",
    "    text='PQRYWTMNVELHSFCIKADG'\n",
    "    N = 1000\n",
    "    X = np.ndarray((M,N))\n",
    "    y = np.ones(M)\n",
    "    y[M1:] = 0\n",
    "    k = 0\n",
    "    phychemDict={}\n",
    "    phychemDict[\"alcohol\"]=(\"S\",\"T\")# 有乙醇基\n",
    "    phychemDict[\"aliphatic\"]=(\"I\",\"L\",\"V\")# 脂肪族\n",
    "    phychemDict[\"aromatic\"]=(\"F\",\"H\",\"W\",\"Y\")# 芳香族\n",
    "    phychemDict[\"charged\"]=(\"D\",\"E\",\"H\",\"K\",\"R\")# 带电性\n",
    "    phychemDict[\"positive\"]=(\"K\",\"H\",\"R\")# 带正电\n",
    "    phychemDict[\"negative\"]=(\"D\",\"E\")# 带负电\n",
    "    phychemDict[\"polar\"]=(\"A\",\"L\",\"I\",\"P\",\"F\",\"W\",\"M\")# 非极性\n",
    "    phychemDict[\"small\"]=(\"A\",\"C\",\"D\",\"G\",\"N\",\"P\",\"S\",\"T\",\"V\")# 小分子\n",
    "    phychemDict[\"turnlike\"]=(\"A\",\"C\",\"D\",\"E\",\"G\",\"H\",\"K\",\"N\",\"Q\",\"R\",\"S\",\"T\")\n",
    "    phychemDict[\"hydrophobic\"]=(\"A\",\"F\",\"I\",\"L\",\"M\",\"P\",\"V\",\"W\",\"Y\")# 疏水\n",
    "    phychemDict[\"asa\"]=(\"A\",\"N\",\"D\",\"C\",\"P\",\"S\",\"T\",\"G\",\"V\")# 可溶解表面积低于平均值\n",
    "    phychemDict[\"pr\"]=(\"F\",\"Y\",\"W\")# 在紫外区有光吸收能力\n",
    "    \n",
    "    keys = phychemDict.keys()\n",
    "    print(\"keys:\",keys)\n",
    "    for file in files:\n",
    "        for seq_record in SeqIO.parse(file,'fasta'):\n",
    "            seq = str(seq_record.seq)\n",
    "            seq = re.sub('[XZUB]',\"\",seq)\n",
    "            c = len(seq)\n",
    "            m = np.zeros((len(seq),20))\n",
    "            for i in range(c):\n",
    "                j = 0\n",
    "                for key in keys:\n",
    "                    val = phychemDict[key]\n",
    "                    if seq[i] in val:\n",
    "                        m[i][j] = 1\n",
    "                    j += 1\n",
    "                \n",
    "            m = m.reshape((1,-1))\n",
    "            #print(\"in {},{}:{}\".format(file,seq_record.id,m.shape))\n",
    "            # 只截取蛋白质序列前50个aa，不足的补0\n",
    "            c = c*20\n",
    "            if c < N:\n",
    "                X[k][:c] = m[0]\n",
    "                X[k][c:] = 0\n",
    "            elif c == N:\n",
    "                X[k] = m[0]\n",
    "            else:\n",
    "                X[k] = m[0][:N] \n",
    "            k += 1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义函数，构建2通道的数据集。通道一是hmmer profile；通道二是AA的ONEHOTE编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTwoChannelsArray():\n",
    "    X1,y = load_hmm_prof()\n",
    "    X2,y = AAOneHot()\n",
    "    X=np.ndarray([M,50,20,2])\n",
    "    X11 = X1.reshape([M,50,20])\n",
    "    X21 = X2.reshape([M,50,20])\n",
    "    X[:,:,:,0]=X11\n",
    "    X[:,:,:,1]=X21\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getThreeChannelsArray():\n",
    "    X1,y = load_hmm_prof()\n",
    "    X2,y = AAOneHot()\n",
    "    X3,y = AAPhyChemOneHot()\n",
    "    X=np.ndarray([M,50,20,3])\n",
    "    X11 = X1.reshape([M,50,20])\n",
    "    X21 = X2.reshape([M,50,20])\n",
    "    X31 = X3.reshape([M,50,20])\n",
    "    X[:,:,:,0]=X11\n",
    "    X[:,:,:,1]=X21\n",
    "    X[:,:,:,2]=X31\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建卷积网络进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alexnet(num_classes,channels=1):\n",
    "    # Building 'AlexNet'\n",
    "    network = input_data(shape=[None, 50, 20, channels])\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,25,10,96]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,13,5,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,7,3,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "    network = regression(network, optimizer='momentum',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cifarnet(num_classes,channels=1):\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "    \n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "    \n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, 50, 20, channels],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = dropout(network, 0.75)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 512, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 512, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    return network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building 'VGG Network'\n",
    "def create_vggnet(num_classes,channels=1):\n",
    "    network = input_data(shape=[None, 50, 20, channels])\n",
    "\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,25,10,64]\n",
    "\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,13,5,128]\n",
    "\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,7,3,256]\n",
    "    \n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    \n",
    "    network = fully_connected(network, 2048, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "\n",
    "    network = regression(network, optimizer='rmsprop',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.01)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义交叉验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jackknife_test(X, y, num_classes=2,channels=1,n_epoch=100):\n",
    "    y_pred = np.zeros(M)\n",
    "    loo = LeaveOneOut()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        print(\"\\r In predicting {}\".format(test_index))\n",
    "        X_train, X_test = X[train_index], [X[test_index]]\n",
    "        y_train, y_test = y[train_index], [y[test_index]]\n",
    "        tf.reset_default_graph()\n",
    "        net = create_alexnet(num_classes,channels)\n",
    "        model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "        model.fit(X_train, y_train, n_epoch, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32, \n",
    "              run_id='AMP_cnn')\n",
    "        y_pred[test_index] = model.predict(X_test)\n",
    "        \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def cross_validate(X,y,n_splits=3,num_classes=2,channels=1,n_epoch=100):\n",
    "    pred_prob = np.zeros([M,2])\n",
    "    kf = KFold(n_splits)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tf.reset_default_graph()\n",
    "        net = create_cifarnet(num_classes,channels)\n",
    "        model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "        model.fit(X_train, y_train, n_epoch, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32)\n",
    "        pred_prob[test_index] = model.predict(X_test)\n",
    "        \n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算准确率acc和可接受曲线下面积AUC\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "def metric(y, predprob):\n",
    "    d1 = len(y)\n",
    "    y_pred = np.zeros((d1,2))\n",
    "    for i in range(d1):\n",
    "        if predprob[i][0] > predprob[i][1]:\n",
    "            y_pred[i][0] = 1\n",
    "        else:\n",
    "            y_pred[i][1] = 1\n",
    "            \n",
    "    accuracy=accuracy_score(y[:,0],y_pred[:,0])\n",
    "    print(\"accuracy={}\".format(accuracy))\n",
    "    fpr,tpr,thresholds=roc_curve(y[:,0],predprob[:,0],pos_label=1)\n",
    "    print(\"AUC={}\".format(auc(fpr,tpr)))\n",
    "    mcc = matthews_corrcoef(y[:,0],y_pred[:,0])\n",
    "    print(\"mcc={}\".format(mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仅hmmer profil一个通道数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,y = load_hmm_prof()\n",
    "X1 = X1.reshape([-1,50,20,1])\n",
    "y = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    y[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    y[i][1] = 1\n",
    "# 交叉验证\n",
    "X1,y = shuffle(X1,y)\n",
    "predprob = cross_validate(X1,y,n_splits=5,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两个通道数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4149  | total loss: \u001b[1m\u001b[32m0.31482\u001b[0m\u001b[0m | time: 15.833s\n",
      "| Adam | epoch: 050 | loss: 0.31482 - acc: 0.8811 -- iter: 2624/2628\n",
      "Training Step: 4150  | total loss: \u001b[1m\u001b[32m0.31048\u001b[0m\u001b[0m | time: 17.157s\n",
      "| Adam | epoch: 050 | loss: 0.31048 - acc: 0.8805 | val_loss: 0.34008 - val_acc: 0.8521 -- iter: 2628/2628\n",
      "--\n",
      "accuracy=0.856272838002436\n",
      "AUC=0.9045101809606928\n"
     ]
    }
   ],
   "source": [
    "# 获取数据\n",
    "X,y = getTwoChannelsArray()\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1\n",
    "# 交叉验证\n",
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=2,n_epoch=50)\n",
    "metric(y, predprob) # print: 0.81375, 0.883389\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> <b>预测结果</b></font><br>\n",
    "5-fold, cifanet,30 epochs, 879amps+2405 not amps, acc=0.8562, auc=0.9045, mcc=0.6470<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三个通道数据训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2489  | total loss: \u001b[1m\u001b[32m0.33754\u001b[0m\u001b[0m | time: 16.733s\n",
      "| Adam | epoch: 030 | loss: 0.33754 - acc: 0.8439 -- iter: 2624/2628\n",
      "Training Step: 2490  | total loss: \u001b[1m\u001b[32m0.32230\u001b[0m\u001b[0m | time: 18.068s\n",
      "| Adam | epoch: 030 | loss: 0.32230 - acc: 0.8533 | val_loss: 0.35791 - val_acc: 0.8659 -- iter: 2628/2628\n",
      "--\n",
      "accuracy=0.8419610231425091\n",
      "AUC=0.8856872887589611\n"
     ]
    }
   ],
   "source": [
    "X,y = getThreeChannelsArray()\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1\n",
    "# 交叉验证\n",
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y, predprob)# accuracy=0.820625 AUC=0.9052437499999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>预测结果</b></font><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其它，包括测试代码段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = net(X_train, y_train, X_test, y_test)\n",
    "predprob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predLabel = model.predict_label([X_test[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1=np.ndarray([3,4])\n",
    "t2=t1.reshape(12)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t3=t1[:2,:].reshape([-1,2])\n",
    "t4=t3.reshape(8)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = AAPhyChemOneHot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "879+2405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "93.7+173.7+250.4+215.2+146.3+197.6+142.6+228.6+135.2+177.7+109.5+182.9+142.1+52.6+271.6+188.1+239.9+182.2+157.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3287.1/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
