{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蛋白质编码\n",
    "\n",
    "读取AMPs和notAMPs序列，转化为2个通道的特征向量表示。其中通道1的数据来自hmmer的profile；通道2的数据来自氨基酸Onehot编码;通道3来自AA的物化性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#jsonfiles = ['./data/benchmark/AMPs_50_hmm_profil.json','./data/benchmark/notAMPs_50_hmm_profil.json']\n",
    "#fastafiles=['./data/benchmark/wpAMPs.fasta','./data/benchmark/wpnotAMPs.fasta']\n",
    "#files=['./data/benchmark/AMPs_50.fasta','./data/benchmark/notAMPs_50.fasta']\n",
    "text='PQRYWTMNVELHSFCIKADG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对矩阵进行归一化\n",
    "def maxminnorm(array):\n",
    "    maxcols=array.max(axis=0)\n",
    "    mincols=array.min(axis=0)\n",
    "    data_shape = array.shape\n",
    "    data_rows = data_shape[0]\n",
    "    data_cols = data_shape[1]\n",
    "    t=np.empty((data_rows,data_cols))\n",
    "    for i in range(data_cols):\n",
    "        if maxcols[i] > mincols[i]:\n",
    "            t[:,i]=(array[:,i]-mincols[i])/(maxcols[i]-mincols[i])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_hmm_prof: 读入序列的hmmer profil 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载来自hmmer profil的数据\n",
    "# jsonfile 存储hmmer profil数据的json格式文件名\n",
    "# fastafile 序列的参照文件，fasta格式\n",
    "# numAA>0 表示从头取numAA个氨基酸的profil，尾不足补0; numAA<0 表示从尾往前取numAA个氨基酸的profil，头不足补0\n",
    "def load_hmm_prof(jsonfile, fastafile, numAA=50):\n",
    "    records = SeqIO.parse(fastafile, 'fasta')\n",
    "    seqID = [str(x.id) for x in records]\n",
    "    records.close()\n",
    "    \n",
    "    M = len(seqID)\n",
    "    N = abs(numAA) * 20\n",
    "    \n",
    "    X = np.ndarray((M,N))\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    fr = open(jsonfile,'r')\n",
    "    p = json.load(fr)\n",
    "    fr.close()\n",
    "    \n",
    "    for key in seqID:\n",
    "        ary = p[key]\n",
    "        tm = np.array(ary).reshape([-1,20])\n",
    "        tm = tm[1:,:]\n",
    "        c = len(ary)-20\n",
    "        if numAA > 0:\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k][:c] = tm.reshape(c)\n",
    "                X[k][c:] = 0\n",
    "            elif c == N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[:numAA,:]\n",
    "                t = maxminnorm(t)# 归一化\n",
    "                X[k] = t.reshape(N)\n",
    "        else:# numAA < 0\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)\n",
    "                X[k][-c:] = tm.reshape(c)\n",
    "                X[k][:-c] = 0\n",
    "            elif c==N:\n",
    "                tm = maxminnorm(tm)\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[numAA:,:]\n",
    "                t = maxminnorm(t)\n",
    "                X[k] = t.reshape(N)\n",
    "\n",
    "        k += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 氨基酸OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numAA=0 蛋白质序列全肽链； numAA>0 从头截止到第numAA个氨基酸； numAA<0 从尾部往前numAA个氨基酸\n",
    "# 返回一个列表，列表的每一行表示一个蛋白质序列的OneHot编码\n",
    "def AAOneHot(fastafile, numAA=50):\n",
    "    X = []\n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "            seq = str(seq_record.seq)\n",
    "            seq = re.sub('[XZUB]',\"\",seq)\n",
    "            c = len(seq)\n",
    "            m = np.zeros([c,20])\n",
    "            for i in range(c):\n",
    "                j = text.index(seq[i])\n",
    "                m[i][j] = 1.0\n",
    "            m = m.reshape([1,-1])\n",
    "            X.append(m)\n",
    "            \n",
    "    if numAA == 0:\n",
    "        return X\n",
    "    elif numAA> 0:\n",
    "        L = numAA*20\n",
    "        A = []\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[:c] = m\n",
    "                t[c:] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[:L]\n",
    "            A.append(t)\n",
    "        return A\n",
    "    else: # numAA < 0\n",
    "        L = abs(numAA)*20\n",
    "        A = []\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[-c:] = m\n",
    "                t[:-c] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[-L:]\n",
    "            A.append(t)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dAAOneHot(): 氨基酸两联体编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 两联体编码\n",
    "def dAAOneHot(fastafile):\n",
    "    daa=[x+y for x in text for y in text]\n",
    "    X = []\n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "        seq = str(seq_record.seq)\n",
    "        seq = re.sub('[XZUB]',\"\",seq)\n",
    "        t = np.zeros(400)\n",
    "        for j in range(400):\n",
    "            t[j] = seq.count(daa[j])\n",
    "        maxv, minv = t.max(),t.min()\n",
    "        for j in range(400):\n",
    "            t[j] = (t[j]-minv)/(maxv-minv)\n",
    "        X.append(t)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 氨基酸物化属性OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numAA=0 蛋白质序列全肽链； numAA>0 从头截止到第numAA个氨基酸； numAA<0 从尾部往前numAA个氨基酸\n",
    "# 返回一个列表，列表的每一行表示一个蛋白质序列的物化属性的OneHot编码\n",
    "def AAPhyChemOneHot(fastafile, numAA):\n",
    "    phychemDict={}\n",
    "    phychemDict[\"alcohol\"]=(\"S\",\"T\")# 有乙醇基\n",
    "    phychemDict[\"aliphatic\"]=(\"I\",\"L\",\"V\")# 脂肪族\n",
    "    phychemDict[\"aromatic\"]=(\"F\",\"H\",\"W\",\"Y\")# 芳香族\n",
    "    phychemDict[\"charged\"]=(\"D\",\"E\",\"H\",\"K\",\"R\")# 带电性\n",
    "    phychemDict[\"positive\"]=(\"K\",\"H\",\"R\")# 带正电\n",
    "    phychemDict[\"negative\"]=(\"D\",\"E\")# 带负电\n",
    "    phychemDict[\"polar\"]=(\"A\",\"L\",\"I\",\"P\",\"F\",\"W\",\"M\")# 非极性\n",
    "    phychemDict[\"small\"]=(\"A\",\"C\",\"D\",\"G\",\"N\",\"P\",\"S\",\"T\",\"V\")# 小分子\n",
    "    phychemDict[\"turnlike\"]=(\"A\",\"C\",\"D\",\"E\",\"G\",\"H\",\"K\",\"N\",\"Q\",\"R\",\"S\",\"T\")\n",
    "    phychemDict[\"hydrophobic\"]=(\"A\",\"F\",\"I\",\"L\",\"M\",\"P\",\"V\",\"W\",\"Y\")# 疏水\n",
    "    phychemDict[\"asa\"]=(\"A\",\"N\",\"D\",\"C\",\"P\",\"S\",\"T\",\"G\",\"V\")# 可溶解表面积低于平均值\n",
    "    phychemDict[\"pr\"]=(\"F\",\"Y\",\"W\")# 在紫外区有光吸收能力\n",
    "       \n",
    "    X = []\n",
    "    keys = phychemDict.keys()\n",
    "    lskey = list(keys)    \n",
    "    N = len(keys)\n",
    "    \n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "        seq = str(seq_record.seq)\n",
    "        seq = re.sub('[XZUB]',\"\",seq)\n",
    "        c = len(seq)\n",
    "        m = np.zeros((len(seq),N))\n",
    "        \n",
    "        for i in range(c):\n",
    "            for j in range(N):\n",
    "                key = lskey[j]\n",
    "                val = phychemDict[key]\n",
    "                if seq[i] in val:\n",
    "                    m[i][j] = 1\n",
    " \n",
    "        m = m.reshape((1,-1))\n",
    "        X.append(m)\n",
    "        \n",
    "    if numAA == 0:\n",
    "        return X\n",
    "    elif numAA > 0:# 只截取蛋白质序列前numAA个氨基酸，不足在尾部补0\n",
    "        A = []\n",
    "        L = numAA*20\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[:c] = m\n",
    "                t[c:] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[:L] \n",
    "            A.append(t)\n",
    "        return A\n",
    "    else: # numAA<0 截取蛋白质序列后numAA个氨基酸，不足在头部补0\n",
    "        A = []\n",
    "        L = abs(numAA)*20\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[-c:] = m\n",
    "                t[:-c] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[-L:]\n",
    "            A.aapend(t)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建卷积网络进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入tensorflow的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Alex网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alexnet(num_classes,channels=1):\n",
    "    # Building 'AlexNet'\n",
    "    network = input_data(shape=[None, 50, 20, channels])\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,10,10,96]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,5,5,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,3,3,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "    network = regression(network, optimizer='momentum',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Cifar网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cifarnet(num_classes,channels=1):\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "    \n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "    \n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, 20, 20, channels],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = dropout(network, 0.75)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 512, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 512, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    return network\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建vgg网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building 'VGG Network'\n",
    "def create_vggnet(num_classes,channels=1):\n",
    "    network = input_data(shape=[None, 100, 20, channels])\n",
    "\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,25,10,64]\n",
    "\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,13,5,128]\n",
    "\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,7,3,256]\n",
    "    \n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    \n",
    "    network = fully_connected(network, 2048, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "\n",
    "    network = regression(network, optimizer='rmsprop',\n",
    "                         loss='softmax_categorical_crossentropy',\n",
    "                         learning_rate=0.0001)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义交叉验证函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jack knife测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 返回预测的结果，预测的结果为各个类的概率\n",
    "def jackknife_test(X, y, num_classes=2,channels=1,n_epoch=100):\n",
    "    M = X.shape[0]\n",
    "    y_pred = np.zeros([M,2])\n",
    "    loo = LeaveOneOut()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        print(\"\\r In predicting {}\".format(test_index))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tf.reset_default_graph()\n",
    "        net = create_alexnet(num_classes,channels)\n",
    "        model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "        model.fit(X_train, y_train, n_epoch, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32, \n",
    "              run_id='AMP_cnn')\n",
    "        y_pred[test_index] = model.predict(X_test)\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold折叠验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 返回预测的结果，预测的结果为各个类的概率\n",
    "def cross_validate(X,y,n_splits=3,num_classes=2,channels=1,n_epoch=100):\n",
    "    M = X.shape[0]\n",
    "    pred_prob = np.zeros([M,2])\n",
    "    kf = KFold(n_splits)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tf.reset_default_graph()\n",
    "        net = create_alexnet(num_classes,channels)\n",
    "        model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "        model.fit(X_train, y_train, n_epoch, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32)\n",
    "        pred_prob[test_index] = model.predict(X_test)\n",
    "        \n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算预测性能（指标：ACC、AUC和MCC）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算准确率acc和可接受曲线下面积AUC\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "def metric(y, predprob):\n",
    "    d1 = len(y)\n",
    "    y_pred = np.zeros((d1,2))\n",
    "    for i in range(d1):\n",
    "        if predprob[i][0] > predprob[i][1]:\n",
    "            y_pred[i][0] = 1\n",
    "        else:\n",
    "            y_pred[i][1] = 1\n",
    "            \n",
    "    accuracy=accuracy_score(y[:,0],y_pred[:,0])\n",
    "    print(\"accuracy={}\".format(accuracy))\n",
    "    fpr,tpr,thresholds=roc_curve(y[:,0],predprob[:,0],pos_label=1)\n",
    "    print(\"AUC={}\".format(auc(fpr,tpr)))\n",
    "    mcc = matthews_corrcoef(y[:,0],y_pred[:,0])\n",
    "    print(\"mcc={}\".format(mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一层预测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用iAMP-2L的数据训练及预测\n",
    "<p/>\n",
    "<font size=4>数据由王普构建，包含879个AMP和2405个非抗菌他</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonfiles_wp = ['./data/benchmark/wpAMPs_hmm_profil.json','./data/benchmark/wpnotAMPs_hmm_profil.json']\n",
    "files_wp=['./data/benchmark/wpAMPs.fasta','./data/benchmark/wpnotAMPs.fasta']\n",
    "N = 20\n",
    "X1_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],N)\n",
    "X1_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],N)\n",
    "X1 = np.vstack((X1_1, X1_2)).reshape([-1,N,20])\n",
    "\n",
    "X2_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],-N)\n",
    "X2_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],-N)\n",
    "X2 = np.vstack((X2_1, X2_2)).reshape([-1,N,20])\n",
    "\n",
    "X3_1 = np.array(dAAOneHot(files_wp[0]))\n",
    "X3_2 = np.array(dAAOneHot(files_wp[1]))\n",
    "X3 = np.vstack((X3_1, X3_2)).reshape([-1,20,20])\n",
    "\n",
    "m1 = X1_1.shape[0]\n",
    "m2 = X1_2.shape[0]\n",
    "\n",
    "X = np.ndarray([m1+m2,20,20,3])\n",
    "X[:,:,:,0] = X1\n",
    "X[:,:,:,1] = X2\n",
    "X[:,:,:,2] = X3\n",
    "\n",
    "y = np.zeros(m1+m2)\n",
    "y[m1:] = 1\n",
    "y = to_categorical(y,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jack-knife 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.56687\u001b[0m\u001b[0m | time: 7.396s\n",
      "\u001b[2K\r",
      "| Momentum | epoch: 004 | loss: 0.56687 - acc: 0.7293 -- iter: 0384/3283\n"
     ]
    }
   ],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = jackknife_test(X,y,2,3,35)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用自己构建的数据集训练和测试\n",
    "<p></p>\n",
    "<font size=4>包含800个AMP和800个非AMP</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二层预测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仅hmmer profil一个通道数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1,y = load_hmm_prof()\n",
    "X1 = X1.reshape([-1,50,20,1])\n",
    "y = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    y[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    y[i][1] = 1\n",
    "# 交叉验证\n",
    "X1,y = shuffle(X1,y)\n",
    "predprob = cross_validate(X1,y,n_splits=5,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两个通道数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "X,y = getTwoChannelsArray()\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1\n",
    "# 交叉验证\n",
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=2,n_epoch=50)\n",
    "metric(y, predprob) # print: 0.81375, 0.883389\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> <b>预测结果</b></font><br>\n",
    "5-fold, cifanet,30 epochs, 879amps+2405 not amps, acc=0.8562, auc=0.9045, mcc=0.6470<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三个通道数据训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = getThreeChannelsArray()\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1\n",
    "# 交叉验证\n",
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y, predprob)# accuracy=0.820625 AUC=0.9052437499999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_prob = np.zeros(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test=X[:1280],X[1280:]\n",
    "y_train,y_test=y[:1280],y[1280:]\n",
    "tf.reset_default_graph()\n",
    "net = create_vggnet(2,3)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, n_epoch=30, shuffle=True, \n",
    "      show_metric=True, batch_size=32)\n",
    "predval = model.predict(X_test)\n",
    "metric(y_test, predval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predval = model.predict(X_test)\n",
    "metric(y_test, predval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>预测结果</b></font><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两联体-前20hmmrof-后20hmmprof,三个通道数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建数据集\n",
    "X1,y1 = load_hmm_prof(20,0)\n",
    "X2,y2 = load_hmm_prof(20,-1)\n",
    "X3,y3 = dAAOneHot()\n",
    "X=np.ndarray([M,20,20,3])\n",
    "X11 = X1.reshape([M,20,20])\n",
    "X21 = X2.reshape([M,20,20])\n",
    "X31 = X3.reshape([M,20,20])\n",
    "X[:,:,:,0]=X11\n",
    "X[:,:,:,1]=X21\n",
    "X[:,:,:,2]=X31\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3319  | total loss: \u001b[1m\u001b[32m0.00074\u001b[0m\u001b[0m | time: 55.461s\n",
      "| Momentum | epoch: 040 | loss: 0.00074 - acc: 1.0000 -- iter: 2624/2628\n",
      "Training Step: 3320  | total loss: \u001b[1m\u001b[32m0.00075\u001b[0m\u001b[0m | time: 58.234s\n",
      "| Momentum | epoch: 040 | loss: 0.00075 - acc: 1.0000 | val_loss: 0.58507 - val_acc: 0.8994 -- iter: 2628/2628\n",
      "--\n",
      "accuracy=0.9095615103532277\n",
      "AUC=0.9577068536112904\n",
      "mcc=0.7656093650437452\n"
     ]
    }
   ],
   "source": [
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=3,n_epoch=40)\n",
    "metric(y, predprob)# 王普的数据，用alexnet, acc=0.90956 auc=0.957706  mcc=0.765609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仅测试代码用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2904  | total loss: \u001b[1m\u001b[32m0.00190\u001b[0m\u001b[0m | time: 46.510s\n",
      "| Momentum | epoch: 035 | loss: 0.00190 - acc: 1.0000 -- iter: 2624/2628\n",
      "Training Step: 2905  | total loss: \u001b[1m\u001b[32m0.00214\u001b[0m\u001b[0m | time: 49.098s\n",
      "| Momentum | epoch: 035 | loss: 0.00214 - acc: 1.0000 | val_loss: 0.65024 - val_acc: 0.8826 -- iter: 2628/2628\n",
      "--\n",
      "accuracy=0.8826219512195121\n",
      "AUC=0.9303817483623221\n",
      "mcc=0.7079099904465954\n"
     ]
    }
   ],
   "source": [
    "X,y = shuffle(X,y)\n",
    "X_train,X_test=X[:-656],X[-656:]\n",
    "y_train,y_test=y[:-656],y[-656:]\n",
    "tf.reset_default_graph()\n",
    "net = create_vggnet(2,3)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, 30, shuffle=True, \n",
    "      validation_set=(X_test,y_test),\n",
    "      show_metric=True, batch_size=32)\n",
    "pred_prob= model.predict(X_test)\n",
    "metric(y_test, pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41271517, 0.58728486],\n",
       "       [0.9371414 , 0.06285849],\n",
       "       [0.00776255, 0.9922375 ],\n",
       "       ...,\n",
       "       [0.08722904, 0.912771  ],\n",
       "       [0.5964559 , 0.40354416],\n",
       "       [0.00463584, 0.9953642 ]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
