{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蛋白质编码\n",
    "\n",
    "读取AMPs和notAMPs序列，转化为2个通道的特征向量表示。其中通道1的数据来自hmmer的profile；通道2的数据来自氨基酸Onehot编码;通道3来自AA的物化性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonfiles = ['./data/benchmark/AMPs_50_hmm_profil.json','./data/benchmark/notAMPs_50_hmm_profil.json']\n",
    "#fastafiles=['./data/benchmark/wpAMPs.fasta','./data/benchmark/wpnotAMPs.fasta']\n",
    "#files=['./data/benchmark/AMPs_50.fasta','./data/benchmark/notAMPs_50.fasta']\n",
    "text='PQRYWTMNVELHSFCIKADG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对矩阵进行归一化\n",
    "def maxminnorm(array):\n",
    "    maxcols=array.max(axis=0)\n",
    "    mincols=array.min(axis=0)\n",
    "    data_shape = array.shape\n",
    "    data_rows = data_shape[0]\n",
    "    data_cols = data_shape[1]\n",
    "    t=np.empty((data_rows,data_cols))\n",
    "    for i in range(data_cols):\n",
    "        if maxcols[i] > mincols[i]:\n",
    "            t[:,i]=(array[:,i]-mincols[i])/(maxcols[i]-mincols[i])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_hmm_prof: 读入序列的hmmer profil 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载来自hmmer profil的数据\n",
    "# jsonfile 存储hmmer profil数据的json格式文件名\n",
    "# fastafile 序列的参照文件，fasta格式\n",
    "# numAA>0 表示从头取numAA个氨基酸的profil，尾不足补0; numAA<0 表示从尾往前取numAA个氨基酸的profil，头不足补0\n",
    "def load_hmm_prof(jsonfile, fastafile, numAA=50):\n",
    "    records = SeqIO.parse(fastafile, 'fasta')\n",
    "    seqID = [str(x.id) for x in records]\n",
    "    records.close()\n",
    "    \n",
    "    M = len(seqID)\n",
    "    N = abs(numAA) * 20\n",
    "    \n",
    "    X = np.ndarray((M,N))\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    fr = open(jsonfile,'r')\n",
    "    p = json.load(fr)\n",
    "    fr.close()\n",
    "    \n",
    "    for key in seqID:\n",
    "        ary = p[key]\n",
    "        tm = np.array(ary).reshape([-1,20])\n",
    "        tm = tm[1:,:]\n",
    "        c = len(ary)-20\n",
    "        if numAA > 0:\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k][:c] = tm.reshape(c)\n",
    "                X[k][c:] = 0\n",
    "            elif c == N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[:numAA,:]\n",
    "                t = maxminnorm(t)# 归一化\n",
    "                X[k] = t.reshape(N)\n",
    "        else:# numAA < 0\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)\n",
    "                X[k][-c:] = tm.reshape(c)\n",
    "                X[k][:-c] = 0\n",
    "            elif c==N:\n",
    "                tm = maxminnorm(tm)\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[numAA:,:]\n",
    "                t = maxminnorm(t)\n",
    "                X[k] = t.reshape(N)\n",
    "\n",
    "        k += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 氨基酸OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numAA=0 蛋白质序列全肽链； numAA>0 从头截止到第numAA个氨基酸； numAA<0 从尾部往前numAA个氨基酸\n",
    "# 返回一个列表，列表的每一行表示一个蛋白质序列的OneHot编码\n",
    "def AAOneHot(fastafile, numAA=50):\n",
    "    X = []\n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "            seq = str(seq_record.seq)\n",
    "            seq = re.sub('[XZUB]',\"\",seq)\n",
    "            c = len(seq)\n",
    "            m = np.zeros([c,20])\n",
    "            for i in range(c):\n",
    "                j = text.index(seq[i])\n",
    "                m[i][j] = 1.0\n",
    "            m = m.reshape([1,-1])\n",
    "            X.append(m)\n",
    "            \n",
    "    if numAA == 0:\n",
    "        return X\n",
    "    elif numAA> 0:\n",
    "        L = numAA*20\n",
    "        A = []\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[:c] = m\n",
    "                t[c:] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[:L]\n",
    "            A.append(t)\n",
    "        return A\n",
    "    else: # numAA < 0\n",
    "        L = abs(numAA)*20\n",
    "        A = []\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[-c:] = m\n",
    "                t[:-c] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[-L:]\n",
    "            A.append(t)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dAAOneHot(): 氨基酸两联体编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两联体编码\n",
    "def dAAOneHot(fastafile):\n",
    "    daa=[x+y for x in text for y in text]\n",
    "    X = []\n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "        seq = str(seq_record.seq)\n",
    "        seq = re.sub('[XZUB]',\"\",seq)\n",
    "        t = np.zeros(400)\n",
    "        for j in range(400):\n",
    "            t[j] = seq.count(daa[j])\n",
    "        s = sum(t)\n",
    "        for j in range(400):\n",
    "            t[j] = t[j]/s\n",
    "        X.append(t)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 氨基酸物化属性OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numAA=0 蛋白质序列全肽链； numAA>0 从头截止到第numAA个氨基酸； numAA<0 从尾部往前numAA个氨基酸\n",
    "# 返回一个列表，列表的每一行表示一个蛋白质序列的物化属性的OneHot编码\n",
    "def AAPhyChemOneHot(fastafile, numAA):\n",
    "    phychemDict={}\n",
    "    phychemDict[\"alcohol\"]=(\"S\",\"T\")# 有乙醇基\n",
    "    phychemDict[\"aliphatic\"]=(\"I\",\"L\",\"V\")# 脂肪族\n",
    "    phychemDict[\"aromatic\"]=(\"F\",\"H\",\"W\",\"Y\")# 芳香族\n",
    "    phychemDict[\"charged\"]=(\"D\",\"E\",\"H\",\"K\",\"R\")# 带电性\n",
    "    phychemDict[\"positive\"]=(\"K\",\"H\",\"R\")# 带正电\n",
    "    phychemDict[\"negative\"]=(\"D\",\"E\")# 带负电\n",
    "    phychemDict[\"polar\"]=(\"A\",\"L\",\"I\",\"P\",\"F\",\"W\",\"M\")# 非极性\n",
    "    phychemDict[\"small\"]=(\"A\",\"C\",\"D\",\"G\",\"N\",\"P\",\"S\",\"T\",\"V\")# 小分子\n",
    "    phychemDict[\"turnlike\"]=(\"A\",\"C\",\"D\",\"E\",\"G\",\"H\",\"K\",\"N\",\"Q\",\"R\",\"S\",\"T\")\n",
    "    phychemDict[\"hydrophobic\"]=(\"A\",\"F\",\"I\",\"L\",\"M\",\"P\",\"V\",\"W\",\"Y\")# 疏水\n",
    "    phychemDict[\"asa\"]=(\"A\",\"N\",\"D\",\"C\",\"P\",\"S\",\"T\",\"G\",\"V\")# 可溶解表面积低于平均值\n",
    "    phychemDict[\"pr\"]=(\"F\",\"Y\",\"W\")# 在紫外区有光吸收能力\n",
    "       \n",
    "    X = []\n",
    "    keys = phychemDict.keys()\n",
    "    lskey = list(keys)    \n",
    "    N = len(keys)\n",
    "    \n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "        seq = str(seq_record.seq)\n",
    "        seq = re.sub('[XZUB]',\"\",seq)\n",
    "        c = len(seq)\n",
    "        m = np.zeros((len(seq),N))\n",
    "        \n",
    "        for i in range(c):\n",
    "            for j in range(N):\n",
    "                key = lskey[j]\n",
    "                val = phychemDict[key]\n",
    "                if seq[i] in val:\n",
    "                    m[i][j] = 1\n",
    " \n",
    "        m = m.reshape((1,-1))\n",
    "        X.append(m)\n",
    "        \n",
    "    if numAA == 0:\n",
    "        return X\n",
    "    elif numAA > 0:# 只截取蛋白质序列前numAA个氨基酸，不足在尾部补0\n",
    "        A = []\n",
    "        L = numAA*20\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[:c] = m\n",
    "                t[c:] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[:L] \n",
    "            A.append(t)\n",
    "        return A\n",
    "    else: # numAA<0 截取蛋白质序列后numAA个氨基酸，不足在头部补0\n",
    "        A = []\n",
    "        L = abs(numAA)*20\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[-c:] = m\n",
    "                t[:-c] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[-L:]\n",
    "            A.aapend(t)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建卷积网络进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入tensorflow的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Alex网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alexnet(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    # Building 'AlexNet'\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels])\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,10,10,96]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,5,5,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,3,3,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "    network = regression(network, optimizer='momentum',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Cifar网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cifarnet(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "    \n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "    \n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = dropout(network, 0.75)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 512, activation='sigmoid')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 512, activation='sigmoid')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, num_classes, activation='sigmoid')\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    return network\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建vgg网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building 'VGG Network'\n",
    "def create_vggnet(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels])\n",
    "\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,25,10,64]\n",
    "\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,13,5,128]\n",
    "\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,7,3,256]\n",
    "    \n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    \n",
    "    network = fully_connected(network, 2048, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "\n",
    "    network = regression(network, optimizer='rmsprop',\n",
    "                         loss='softmax_categorical_crossentropy',\n",
    "                         learning_rate=0.0001)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其它网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net1(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels])\n",
    "    network = fully_connected(network,128,activation='sigmoid',\n",
    "                             regularizer='L2',\n",
    "                             weight_decay=0.001)\n",
    "    network = fully_connected(network,256,activation='sigmoid',\n",
    "                             regularizer='L2',\n",
    "                             weight_decay=0.001)\n",
    "    #network = fully_connected(network,360,activation='relu',\n",
    "    #                         regularizer='L2',\n",
    "    #                         weight_decay=0.001)\n",
    "    network = dropout(network,0.8)\n",
    "    network = fully_connected(network, num_classes, activation=\"sigmoid\")\n",
    "    network = regression(network, optimizer='adam',\n",
    "                        loss=\"binary_crossentropy\")\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "def cnn_1d(n_features,num_classes):\n",
    "    network = input_data(shape=[None, n_features], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=10000, output_dim=128)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多标记学习的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义交叉验证函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jack knife测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回预测的结果，预测的结果为各个类的概率\n",
    "def jackknife_test(X, y, imgrow=20, imgcol=20, num_classes=2,channels=3,n_epoch=100):\n",
    "    M = X.shape[0]\n",
    "    y_pred = np.zeros([M,2])\n",
    "    loo = LeaveOneOut()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        print(\"\\r In predicting {}\".format(test_index))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tf.reset_default_graph()\n",
    "        net = create_alexnet(imgrow, imgcol, num_classes,channels)\n",
    "        model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "        model.fit(X_train, y_train, n_epoch, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32, \n",
    "              run_id='AMP_cnn')\n",
    "        pred_prob[test_index] = model.predict(X_test)\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold折叠验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回预测的结果，预测的结果为各个类的概率\n",
    "def cross_validate(X,y,n_splits=3,imgrow=20, imgcol=20, num_classes=2, channels=3, n_epoch=100):\n",
    "    M = X.shape[0]\n",
    "    pred_prob = np.zeros([M,2])\n",
    "    kf = KFold(n_splits)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tf.reset_default_graph()\n",
    "        net = create_alexnet(imgrow, imgcol, num_classes, channels)\n",
    "        model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "        model.fit(X_train, y_train, n_epoch, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32)\n",
    "        pred_prob[test_index] = model.predict(X_test)\n",
    "        \n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算预测性能（指标：ACC、AUC和MCC）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算准确率acc和可接受曲线下面积AUC\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "def metric(y, predprob):\n",
    "    d1 = len(y)\n",
    "    y_pred = np.zeros((d1,2))\n",
    "    for i in range(d1):\n",
    "        if predprob[i][0] > predprob[i][1]:\n",
    "            y_pred[i][0] = 1\n",
    "        else:\n",
    "            y_pred[i][1] = 1\n",
    "            \n",
    "    accuracy=accuracy_score(y[:,0],y_pred[:,0])\n",
    "    print(\"accuracy={}\".format(accuracy))\n",
    "    fpr,tpr,thresholds=roc_curve(y[:,0],predprob[:,0],pos_label=1)\n",
    "    print(\"AUC={}\".format(auc(fpr,tpr)))\n",
    "    mcc = matthews_corrcoef(y[:,0],y_pred[:,0])\n",
    "    print(\"mcc={}\".format(mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 伪氨基酸成分数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载arff文件，读入879个AMps和2405个非AMPs样本的特征\n",
    "from scipy.io import arff\n",
    "data,meta = arff.loadarff('./data/benchmark/wp_amp_notamp_30features.arff')\n",
    "n = len(data)\n",
    "X = np.ndarray((n,30))\n",
    "Y = np.zeros(n)\n",
    "for i in range(n):\n",
    "    d = data[i]\n",
    "    for j in range(30):\n",
    "        X[i][j] = float(d[j])\n",
    "    Y[i] = abs(int(d[-1])-1)\n",
    "y = to_categorical(Y,2)\n",
    "X1,y = shuffle(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predprob = cross_validate(X1,y,n_splits=5,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predprob = jackknife_test(X1,y,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一层预测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用iAMP-2L的数据训练及预测\n",
    "<p/>\n",
    "<font size=4>数据由王普构建，包含879个AMP和2405个非抗菌他</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles_wp = ['./data/benchmark/wpAMPs_hmm_profil.json','./data/benchmark/wpnotAMPs_hmm_profil.json']\n",
    "files_wp=['./data/benchmark/wpAMPs.fasta','./data/benchmark/wpnotAMPs.fasta']\n",
    "N = 20\n",
    "X1_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],N)\n",
    "X1_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],N)\n",
    "X1 = np.vstack((X1_1, X1_2)).reshape([-1,N,20])\n",
    "\n",
    "X2_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],-N)\n",
    "X2_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],-N)\n",
    "X2 = np.vstack((X2_1, X2_2)).reshape([-1,N,20])\n",
    "\n",
    "m1 = X1_1.shape[0]\n",
    "m2 = X1_2.shape[0]\n",
    "\n",
    "if N > 20:\n",
    "    t = np.array(dAAOneHot(files_wp[0]))\n",
    "    X3_1 = np.zeros([m1,N,20])\n",
    "    X3_1[:,:20,:] = t.reshape([-1,20,20])\n",
    "    t = np.array(dAAOneHot(files_wp[1]))\n",
    "    X3_2 = np.zeros([m2,N,20])\n",
    "    X3_2[:,:20,:] = t.reshape([-1,20,20])\n",
    "    X3 = np.vstack((X3_1, X3_2))\n",
    "else:\n",
    "    X3_1 = np.array(dAAOneHot(files_wp[0]))\n",
    "    X3_2 = np.array(dAAOneHot(files_wp[1]))\n",
    "    X3 = np.vstack((X3_1, X3_2)).reshape([-1,N,20])\n",
    "    \n",
    "X = np.ndarray([m1+m2,N,20,3])\n",
    "X[:,:,:,0] = X1\n",
    "X[:,:,:,1] = X2\n",
    "X[:,:,:,2] = X3\n",
    "\n",
    "y = np.zeros(m1+m2)\n",
    "y[m1:] = 1\n",
    "y = to_categorical(y,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jack-knife 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = jackknife_test(X,y,20,20,2,3,30)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = cross_validate(X,y,n_splits=10,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><strong>预测结果：</strong></font>\n",
    " - n_splits=10,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30,alexnet:<br>\n",
    " accuracy=0.8815, AUC=0.9325,mcc=0.6993<br>\n",
    " - n_splits=10,imgrow=50,imgcol=20,num_classes=2,channels=3,n_epoch=30:<br>\n",
    " accuracy=0.8846, AUC=0.9428, mcc=0.7079<br> \n",
    " - n_splits=100,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30,alexnet:<br>\n",
    " accuracy=0.889768574908648, AUC=0.9450779211871362,mcc=0.7182219182285516<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用自己构建的数据集训练和测试\n",
    "<p></p>\n",
    "<font size=4>包含800个AMP和800个非AMP</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 构建训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles_wp = ['./data/benchmark/AMPs_50_hmm_profil.json','./data/benchmark/notAMPs_50_hmm_profil.json']\n",
    "files_wp=['./data/benchmark/AMPs_50.fasta','./data/benchmark/notAMPs_50.fasta']\n",
    "N = 20\n",
    "X1_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],N)\n",
    "X1_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],N)\n",
    "X1 = np.vstack((X1_1, X1_2)).reshape([-1,N,20])\n",
    "\n",
    "X2_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],-N)\n",
    "X2_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],-N)\n",
    "X2 = np.vstack((X2_1, X2_2)).reshape([-1,N,20])\n",
    "\n",
    "X3_1 = np.array(dAAOneHot(files_wp[0]))\n",
    "X3_2 = np.array(dAAOneHot(files_wp[1]))\n",
    "X3 = np.vstack((X3_1, X3_2)).reshape([-1,20,20])\n",
    "\n",
    "m1 = X1_1.shape[0]\n",
    "m2 = X1_2.shape[0]\n",
    "\n",
    "X = np.ndarray([m1+m2,20,20,3])\n",
    "X[:,:,:,0] = X1\n",
    "X[:,:,:,1] = X2\n",
    "X[:,:,:,2] = X3\n",
    "\n",
    "y = np.zeros(m1+m2)\n",
    "y[m1:] = 1\n",
    "y = to_categorical(y,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jack-knife测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = jackknife_test(X,y,2,3,35)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二层预测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入多标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = ['antiB', 'antiC', 'antiF', 'antiH', 'antiP', 'antiV',\n",
    "            'antiA', 'antiD', 'antiE', 'antiI', 'antiO', 'antiS', 'antiT', 'antiW', 'surface', 'taxis']\n",
    "#include_T = ['antiB', 'antiC', 'antiF', 'antiH', 'antiP', 'antiV']\n",
    "#exclude_T = ['antiA', 'antiD', 'anitE', 'antiI', 'antiO', 'antiS', 'antiT', 'antiW', 'surface', 'taxis' ]\n",
    "targetFile = './data/benchmark/amps_60_Targets.json'\n",
    "seqFile = './data/benchmark/amps_60_Sequence.json'\n",
    "include_fastafile = './data/benchmark/include_amps_60.fasta'\n",
    "exclude_fastafile = './data/benchmark/exclude_amps_60.fasta'\n",
    "\n",
    "seq_recorders = SeqIO.parse(include_fastafile, 'fasta')\n",
    "seq_ids = [str(r.id) for r in seq_recorders]\n",
    "seq_recorders.close()\n",
    "\n",
    "M = len(seq_ids)\n",
    "\n",
    "# 构建标签数据。共分6类，是include_T中包含的标签。\n",
    "# 构建一个6-by-6的矩阵，\n",
    "ft = open(targetFile,'r')\n",
    "targets = json.load(ft)\n",
    "ft.close()\n",
    "\n",
    "y = np.zeros([M,6])\n",
    "j = 0\n",
    "for key in seq_ids:\n",
    "    keys = targets[key]\n",
    "    for k in keys:\n",
    "        i = families.index(k)\n",
    "        if i < 6:\n",
    "            y[j][i] = 1\n",
    "    j+=1\n",
    "\n",
    "X1 = load_hmm_prof('./data/benchmark/includeAMPs_hmm_profil.json',include_fastafile,20)\n",
    "X2 = load_hmm_prof('./data/benchmark/includeAMPs_hmm_profil.json',include_fastafile,-20)\n",
    "X3 = np.array(dAAOneHot(include_fastafile))\n",
    "\n",
    "X = np.ndarray([X1.shape[0],20,20,3])\n",
    "X[:,:,:,0] = X1.reshape([-1,20,20])\n",
    "X[:,:,:,1] = X2.reshape([-1,20,20])\n",
    "X[:,:,:,2] = X3.reshape([-1,20,20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jack-knife测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = cross_validate(X,y,n_splits=10,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "X_train,X_test=X[:1100],X[1100:]\n",
    "y_train,y_test=y[:1100],y[1100:]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    XX = tf.placeholder(\"float\",[None, 20, 20, 3])\n",
    "    YY = tf.placeholder(\"float\",[None,6])\n",
    "    \n",
    "    # Using TFLearn wrappers for network building\n",
    "    network = conv_2d(XX, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,10,10,96]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,5,5,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,3,3,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    net = fully_connected(network, 6, activation='sigmoid')\n",
    "    \n",
    "    with tf.name_scope('Summaries'):\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=net, labels=YY)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        accuracy = tf.reduce_mean(tf.reduce_sum(loss,axis=1), name=\"acc\")\n",
    "    \n",
    "    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n",
    "                          metric=accuracy, batch_size=64)\n",
    "    model = tflearn.Trainer(train_ops=trainop, \n",
    "                            tensorboard_dir='/tmp/tflearn_logs/',\n",
    "                            tensorboard_verbose=3)\n",
    "    model.fit(feed_dicts={XX: X_train, YY:y_train}, val_feed_dicts={XX:X_test, YY:y_test},\n",
    "           n_epoch=5, show_metric=True, run_id='multilabel_test')\n",
    "    \n",
    "    pred_prob = model.predict(X_test)\n",
    "    pred_label = model.predict_label(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,6]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,6], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_1', defined at:\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-e7d252a737ed>\", line 6, in <module>\n    YY = tf.placeholder(\"float\", [None, 6])\n  File \"/home/weizhong/.local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/weizhong/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/weizhong/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/weizhong/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/weizhong/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,6]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,6], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,6]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,6], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e7d252a737ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     trainer = tflearn.Trainer(train_ops=trainop,\n\u001b[1;32m     48\u001b[0m                               \u001b[0mtensorboard_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/tmp/tflearn_logs/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                               tensorboard_verbose=2)\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Training for 10 epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     trainer.fit({XX: X_train, YY: y_train}, val_feed_dicts={XX: X_test, YY: y_test},\n",
      "\u001b[0;32m~/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_ops, graph, clip_gradients, tensorboard_dir, tensorboard_verbose, checkpoint_path, best_checkpoint_path, max_checkpoints, keep_checkpoint_every_n_hours, random_seed, session, best_val_accuracy)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;31m# Fix for re-using sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m#initialize_uninit_variables(self.session)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,6]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,6], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_1', defined at:\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/weizhong/software/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-e7d252a737ed>\", line 6, in <module>\n    YY = tf.placeholder(\"float\", [None, 6])\n  File \"/home/weizhong/.local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/weizhong/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/weizhong/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/weizhong/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/weizhong/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,6]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,6], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# Define a dnn using Tensorflow\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Model variables\n",
    "    XX = tf.placeholder(\"float\", [None, 20,20,3])\n",
    "    YY = tf.placeholder(\"float\", [None, 6])\n",
    "\n",
    "    # Using TFLearn wrappers for network building\n",
    "    net = tf.reshape(XX, [-1, 20, 20, 3])\n",
    "    net = tflearn.conv_2d(net, 32, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    net = tflearn.local_response_normalization(net)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.conv_2d(net, 64, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    net = tflearn.local_response_normalization(net)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 128, activation='tanh')\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    #net = tflearn.fully_connected(net, 256, activation='tanh')\n",
    "    #net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 6, activation='sigmoid')\n",
    "    \n",
    "    with tf.name_scope('Summaries'):\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=net,labels=YY))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(YY, 1)), tf.float32),\n",
    "            name=\"acc\")\n",
    "\n",
    "    # construct two varaibles to add as additional \"valiation monitors\"\n",
    "    # these varaibles are evaluated each time validation happens (eg at a snapshot)\n",
    "    # and the results are summarized and output to the tensorboard events file,\n",
    "    # together with the accuracy and loss plots.\n",
    "    #\n",
    "    # Here, we generate a dummy variable given by the sum over the current\n",
    "    # network tensor, and a constant variable.  In practice, the validation\n",
    "    # monitor may present useful information, like confusion matrix\n",
    "    # entries, or an AUC metric.\n",
    "    #with tf.name_scope('CustomMonitor'):\n",
    "    #    test_var = tf.reduce_sum(tf.cast(net, tf.float32), name=\"test_var\")\n",
    "    #    test_const = tf.constant(32.0, name=\"custom_constant\")\n",
    "        # Define a train op\n",
    "    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n",
    "                           # validation_monitors=[test_var, test_const],\n",
    "                            metric=accuracy, batch_size=128)\n",
    "\n",
    "    # Tensorboard logs stored in /tmp/tflearn_logs/. Using verbose level 2.\n",
    "    trainer = tflearn.Trainer(train_ops=trainop,\n",
    "                              tensorboard_dir='/tmp/tflearn_logs/',\n",
    "                              tensorboard_verbose=2)\n",
    "    # Training for 10 epochs.\n",
    "    trainer.fit({XX: X_train, YY: y_train}, val_feed_dicts={XX: X_test, YY: y_test},\n",
    "                n_epoch=1, show_metric=True, run_id='Summaries_example')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仅hmmer profil一个通道数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,y = load_hmm_prof()\n",
    "X1 = X1.reshape([-1,50,20,1])\n",
    "y = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    y[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    y[i][1] = 1\n",
    "# 交叉验证\n",
    "X1,y = shuffle(X1,y)\n",
    "predprob = cross_validate(X1,y,n_splits=5,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两个通道数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "X,y = getTwoChannelsArray()\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1\n",
    "# 交叉验证\n",
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=2,n_epoch=50)\n",
    "metric(y, predprob) # print: 0.81375, 0.883389\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> <b>预测结果</b></font><br>\n",
    "5-fold, cifanet,30 epochs, 879amps+2405 not amps, acc=0.8562, auc=0.9045, mcc=0.6470<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三个通道数据训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = getThreeChannelsArray()\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1\n",
    "# 交叉验证\n",
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y, predprob)# accuracy=0.820625 AUC=0.9052437499999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = np.zeros(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test=X[:1280],X[1280:]\n",
    "y_train,y_test=y[:1280],y[1280:]\n",
    "tf.reset_default_graph()\n",
    "net = create_vggnet(2,3)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, n_epoch=30, shuffle=True, \n",
    "      show_metric=True, batch_size=32)\n",
    "predval = model.predict(X_test)\n",
    "metric(y_test, predval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predval = model.predict(X_test)\n",
    "metric(y_test, predval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>预测结果</b></font><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两联体-前20hmmrof-后20hmmprof,三个通道数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据集\n",
    "X1,y1 = load_hmm_prof(20,0)\n",
    "X2,y2 = load_hmm_prof(20,-1)\n",
    "X3,y3 = dAAOneHot()\n",
    "X=np.ndarray([M,20,20,3])\n",
    "X11 = X1.reshape([M,20,20])\n",
    "X21 = X2.reshape([M,20,20])\n",
    "X31 = X3.reshape([M,20,20])\n",
    "X[:,:,:,0]=X11\n",
    "X[:,:,:,1]=X21\n",
    "X[:,:,:,2]=X31\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=3,n_epoch=40)\n",
    "metric(y, predprob)# 王普的数据，用alexnet, acc=0.90956 auc=0.957706  mcc=0.765609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仅测试代码用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "net = create_cifarnet(2,3)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, 1, shuffle=True, \n",
    "      validation_set=(X_test,y_test),\n",
    "      show_metric=True, batch_size=32)\n",
    "pred_prob= model.predict(X_test)\n",
    "metric(y_test, pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    print(\"NO. {} function has {} samples, {}%\".format(i,sum(y[:,i]), 100*sum(y[:,i])/1378))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.zeros([10,5,4])\n",
    "x1=np.ones([10,2,4])\n",
    "x[:,:2,:]=x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
