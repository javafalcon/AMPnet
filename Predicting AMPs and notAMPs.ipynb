{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蛋白质编码\n",
    "\n",
    "读取AMPs和notAMPs序列，转化为2个通道的特征向量表示。其中通道1的数据来自hmmer的profile；通道2的数据来自氨基酸Onehot编码;通道3来自AA的物化性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonfiles = ['./data/benchmark/AMPs_50_hmm_profil.json','./data/benchmark/notAMPs_50_hmm_profil.json']\n",
    "#fastafiles=['./data/benchmark/wpAMPs.fasta','./data/benchmark/wpnotAMPs.fasta']\n",
    "#files=['./data/benchmark/AMPs_50.fasta','./data/benchmark/notAMPs_50.fasta']\n",
    "text='PQRYWTMNVELHSFCIKADG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对矩阵进行归一化\n",
    "def maxminnorm(array):\n",
    "    maxcols=array.max(axis=0)\n",
    "    mincols=array.min(axis=0)\n",
    "    data_shape = array.shape\n",
    "    data_rows = data_shape[0]\n",
    "    data_cols = data_shape[1]\n",
    "    t=np.empty((data_rows,data_cols))\n",
    "    for i in range(data_cols):\n",
    "        if maxcols[i] > mincols[i]:\n",
    "            t[:,i]=(array[:,i]-mincols[i])/(maxcols[i]-mincols[i])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_hmm_prof: 读入序列的hmmer profil 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载来自hmmer profil的数据\n",
    "# jsonfile 存储hmmer profil数据的json格式文件名\n",
    "# fastafile 序列的参照文件，fasta格式\n",
    "# numAA>0 表示从头取numAA个氨基酸的profil，尾不足补0; numAA<0 表示从尾往前取numAA个氨基酸的profil，头不足补0\n",
    "def load_hmm_prof(jsonfile, fastafile, numAA=50):\n",
    "    records = SeqIO.parse(fastafile, 'fasta')\n",
    "    seqID = [str(x.id) for x in records]\n",
    "    records.close()\n",
    "    \n",
    "    M = len(seqID)\n",
    "    N = abs(numAA) * 20\n",
    "    \n",
    "    X = np.ndarray((M,N))\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    fr = open(jsonfile,'r')\n",
    "    p = json.load(fr)\n",
    "    fr.close()\n",
    "    \n",
    "    for key in seqID:\n",
    "        ary = p[key]\n",
    "        tm = np.array(ary).reshape([-1,20])\n",
    "        tm = tm[1:,:]\n",
    "        c = len(ary)-20\n",
    "        if numAA > 0:\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k][:c] = tm.reshape(c)\n",
    "                X[k][c:] = 0\n",
    "            elif c == N:\n",
    "                tm = maxminnorm(tm)# 归一化\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[:numAA,:]\n",
    "                t = maxminnorm(t)# 归一化\n",
    "                X[k] = t.reshape(N)\n",
    "        else:# numAA < 0\n",
    "            if c < N:\n",
    "                tm = maxminnorm(tm)\n",
    "                X[k][-c:] = tm.reshape(c)\n",
    "                X[k][:-c] = 0\n",
    "            elif c==N:\n",
    "                tm = maxminnorm(tm)\n",
    "                X[k] = tm.reshape(c)\n",
    "            else:\n",
    "                t = tm[numAA:,:]\n",
    "                t = maxminnorm(t)\n",
    "                X[k] = t.reshape(N)\n",
    "\n",
    "        k += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 氨基酸OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numAA=0 蛋白质序列全肽链； numAA>0 从头截止到第numAA个氨基酸； numAA<0 从尾部往前numAA个氨基酸\n",
    "# 返回一个列表，列表的每一行表示一个蛋白质序列的OneHot编码\n",
    "def AAOneHot(fastafile, numAA=50):\n",
    "    X = []\n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "            seq = str(seq_record.seq)\n",
    "            seq = re.sub('[XZUB]',\"\",seq)\n",
    "            c = len(seq)\n",
    "            m = np.zeros([c,20])\n",
    "            for i in range(c):\n",
    "                j = text.index(seq[i])\n",
    "                m[i][j] = 1.0\n",
    "            m = m.reshape([1,-1])\n",
    "            X.append(m)\n",
    "            \n",
    "    if numAA == 0:\n",
    "        return X\n",
    "    elif numAA> 0:\n",
    "        L = numAA*20\n",
    "        A = []\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[:c] = m\n",
    "                t[c:] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[:L]\n",
    "            A.append(t)\n",
    "        return A\n",
    "    else: # numAA < 0\n",
    "        L = abs(numAA)*20\n",
    "        A = []\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[-c:] = m\n",
    "                t[:-c] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[-L:]\n",
    "            A.append(t)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dAAOneHot(): 氨基酸两联体编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两联体编码\n",
    "def dAAOneHot(fastafile):\n",
    "    daa=[x+y for x in text for y in text]\n",
    "    X = []\n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "        seq = str(seq_record.seq)\n",
    "        seq = re.sub('[XZUB]',\"\",seq)\n",
    "        t = np.zeros(400)\n",
    "        for j in range(400):\n",
    "            t[j] = seq.count(daa[j])\n",
    "        s = sum(t)\n",
    "        for j in range(400):\n",
    "            t[j] = t[j]/s\n",
    "        X.append(t)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 氨基酸物化属性OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numAA=0 蛋白质序列全肽链； numAA>0 从头截止到第numAA个氨基酸； numAA<0 从尾部往前numAA个氨基酸\n",
    "# 返回一个列表，列表的每一行表示一个蛋白质序列的物化属性的OneHot编码\n",
    "def AAPhyChemOneHot(fastafile, numAA):\n",
    "    phychemDict={}\n",
    "    phychemDict[\"alcohol\"]=(\"S\",\"T\")# 有乙醇基\n",
    "    phychemDict[\"aliphatic\"]=(\"I\",\"L\",\"V\")# 脂肪族\n",
    "    phychemDict[\"aromatic\"]=(\"F\",\"H\",\"W\",\"Y\")# 芳香族\n",
    "    phychemDict[\"charged\"]=(\"D\",\"E\",\"H\",\"K\",\"R\")# 带电性\n",
    "    phychemDict[\"positive\"]=(\"K\",\"H\",\"R\")# 带正电\n",
    "    phychemDict[\"negative\"]=(\"D\",\"E\")# 带负电\n",
    "    phychemDict[\"polar\"]=(\"A\",\"L\",\"I\",\"P\",\"F\",\"W\",\"M\")# 非极性\n",
    "    phychemDict[\"small\"]=(\"A\",\"C\",\"D\",\"G\",\"N\",\"P\",\"S\",\"T\",\"V\")# 小分子\n",
    "    phychemDict[\"turnlike\"]=(\"A\",\"C\",\"D\",\"E\",\"G\",\"H\",\"K\",\"N\",\"Q\",\"R\",\"S\",\"T\")\n",
    "    phychemDict[\"hydrophobic\"]=(\"A\",\"F\",\"I\",\"L\",\"M\",\"P\",\"V\",\"W\",\"Y\")# 疏水\n",
    "    phychemDict[\"asa\"]=(\"A\",\"N\",\"D\",\"C\",\"P\",\"S\",\"T\",\"G\",\"V\")# 可溶解表面积低于平均值\n",
    "    phychemDict[\"pr\"]=(\"F\",\"Y\",\"W\")# 在紫外区有光吸收能力\n",
    "       \n",
    "    X = []\n",
    "    keys = phychemDict.keys()\n",
    "    lskey = list(keys)    \n",
    "    N = len(keys)\n",
    "    \n",
    "    for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "        seq = str(seq_record.seq)\n",
    "        seq = re.sub('[XZUB]',\"\",seq)\n",
    "        c = len(seq)\n",
    "        m = np.zeros((len(seq),N))\n",
    "        \n",
    "        for i in range(c):\n",
    "            for j in range(N):\n",
    "                key = lskey[j]\n",
    "                val = phychemDict[key]\n",
    "                if seq[i] in val:\n",
    "                    m[i][j] = 1\n",
    " \n",
    "        m = m.reshape((1,-1))\n",
    "        X.append(m)\n",
    "        \n",
    "    if numAA == 0:\n",
    "        return X\n",
    "    elif numAA > 0:# 只截取蛋白质序列前numAA个氨基酸，不足在尾部补0\n",
    "        A = []\n",
    "        L = numAA*20\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[:c] = m\n",
    "                t[c:] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[:L] \n",
    "            A.append(t)\n",
    "        return A\n",
    "    else: # numAA<0 截取蛋白质序列后numAA个氨基酸，不足在头部补0\n",
    "        A = []\n",
    "        L = abs(numAA)*20\n",
    "        for m in X:\n",
    "            c = m.size()\n",
    "            t = []\n",
    "            if c < L:\n",
    "                t[-c:] = m\n",
    "                t[:-c] = np.zeros(L-c)\n",
    "            elif c == L:\n",
    "                t = m\n",
    "            else:\n",
    "                t = m[-L:]\n",
    "            A.aapend(t)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建卷积网络进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入tensorflow的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Alex网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alexnet(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    # Building 'AlexNet'\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels])\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,10,10,96]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,5,5,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,3,3,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "    network = regression(network, optimizer='momentum',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Cifar网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cifarnet(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "    \n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "    \n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = dropout(network, 0.75)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 512, activation='sigmoid')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 512, activation='sigmoid')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, num_classes, activation='sigmoid')\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    return network\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建vgg网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building 'VGG Network'\n",
    "def create_vggnet(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels])\n",
    "\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,25,10,64]\n",
    "\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,13,5,128]\n",
    "\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)#[-1,7,3,256]\n",
    "    \n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    \n",
    "    network = fully_connected(network, 2048, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "\n",
    "    network = regression(network, optimizer='rmsprop',\n",
    "                         loss='softmax_categorical_crossentropy',\n",
    "                         learning_rate=0.0001)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其它网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net1(imgrow=20, imgcol=20, num_classes=2, channels=3):\n",
    "    network = input_data(shape=[None, imgrow, imgcol, channels])\n",
    "    network = fully_connected(network,128,activation='sigmoid',\n",
    "                             regularizer='L2',\n",
    "                             weight_decay=0.001)\n",
    "    network = fully_connected(network,256,activation='sigmoid',\n",
    "                             regularizer='L2',\n",
    "                             weight_decay=0.001)\n",
    "    #network = fully_connected(network,360,activation='relu',\n",
    "    #                         regularizer='L2',\n",
    "    #                         weight_decay=0.001)\n",
    "    network = dropout(network,0.8)\n",
    "    network = fully_connected(network, num_classes, activation=\"sigmoid\")\n",
    "    network = regression(network, optimizer='adam',\n",
    "                        loss=\"binary_crossentropy\")\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "def cnn_1d(n_features,num_classes):\n",
    "    network = input_data(shape=[None, n_features], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=10000, output_dim=128)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_classes, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多标记学习的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义交叉验证函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jack knife测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 返回预测的结果，预测的结果为各个类的概率\n",
    "def jackknife_test(X, y, imgrow=20, imgcol=20, num_classes=2,channels=3,n_epoch=100):\n",
    "    M = X.shape[0]\n",
    "    y_pred = np.zeros([M,2])\n",
    "    loo = LeaveOneOut()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        print(\"\\r In predicting {}\".format(test_index))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tf.reset_default_graph()\n",
    "        net = create_alexnet(imgrow, imgcol, num_classes,channels)\n",
    "        model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "        model.fit(X_train, y_train, n_epoch, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32, \n",
    "              run_id='AMP_cnn')\n",
    "        pred_prob[test_index] = model.predict(X_test)\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold折叠验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 返回预测的结果，预测的结果为各个类的概率\n",
    "def cross_validate(X,y,n_splits=3,imgrow=20, imgcol=20, num_classes=2, channels=3, n_epoch=100):\n",
    "    M = X.shape[0]\n",
    "    pred_prob = np.zeros([M,2])\n",
    "    kf = KFold(n_splits)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tf.reset_default_graph()\n",
    "        net = create_alexnet(imgrow, imgcol, num_classes, channels)\n",
    "        model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "        model.fit(X_train, y_train, n_epoch, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32)\n",
    "        pred_prob[test_index] = model.predict(X_test)\n",
    "        \n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算预测性能（指标：ACC、AUC和MCC）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算准确率acc和可接受曲线下面积AUC\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "def metric(y, predprob):\n",
    "    d1 = len(y)\n",
    "    y_pred = np.zeros((d1,2))\n",
    "    for i in range(d1):\n",
    "        if predprob[i][0] > predprob[i][1]:\n",
    "            y_pred[i][0] = 1\n",
    "        else:\n",
    "            y_pred[i][1] = 1\n",
    "            \n",
    "    accuracy=accuracy_score(y[:,0],y_pred[:,0])\n",
    "    print(\"accuracy={}\".format(accuracy))\n",
    "    fpr,tpr,thresholds=roc_curve(y[:,0],predprob[:,0],pos_label=1)\n",
    "    print(\"AUC={}\".format(auc(fpr,tpr)))\n",
    "    mcc = matthews_corrcoef(y[:,0],y_pred[:,0])\n",
    "    print(\"mcc={}\".format(mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 伪氨基酸成分数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载arff文件，读入879个AMps和2405个非AMPs样本的特征\n",
    "from scipy.io import arff\n",
    "data,meta = arff.loadarff('./data/benchmark/wp_amp_notamp_30features.arff')\n",
    "n = len(data)\n",
    "X = np.ndarray((n,30))\n",
    "Y = np.zeros(n)\n",
    "for i in range(n):\n",
    "    d = data[i]\n",
    "    for j in range(30):\n",
    "        X[i][j] = float(d[j])\n",
    "    Y[i] = abs(int(d[-1])-1)\n",
    "y = to_categorical(Y,2)\n",
    "X1,y = shuffle(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predprob = cross_validate(X1,y,n_splits=5,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predprob = jackknife_test(X1,y,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一层预测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用iAMP-2L的数据训练及预测\n",
    "<p/>\n",
    "<font size=4>数据由王普构建，包含879个AMP和2405个非抗菌他</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonfiles_wp = ['./data/benchmark/wpAMPs_hmm_profil.json','./data/benchmark/wpnotAMPs_hmm_profil.json']\n",
    "files_wp=['./data/benchmark/wpAMPs.fasta','./data/benchmark/wpnotAMPs.fasta']\n",
    "N = 20\n",
    "X1_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],N)\n",
    "X1_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],N)\n",
    "X1 = np.vstack((X1_1, X1_2)).reshape([-1,N,20])\n",
    "\n",
    "X2_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],-N)\n",
    "X2_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],-N)\n",
    "X2 = np.vstack((X2_1, X2_2)).reshape([-1,N,20])\n",
    "\n",
    "m1 = X1_1.shape[0]\n",
    "m2 = X1_2.shape[0]\n",
    "\n",
    "if N > 20:\n",
    "    t = np.array(dAAOneHot(files_wp[0]))\n",
    "    X3_1 = np.zeros([m1,N,20])\n",
    "    X3_1[:,:20,:] = t.reshape([-1,20,20])\n",
    "    t = np.array(dAAOneHot(files_wp[1]))\n",
    "    X3_2 = np.zeros([m2,N,20])\n",
    "    X3_2[:,:20,:] = t.reshape([-1,20,20])\n",
    "    X3 = np.vstack((X3_1, X3_2))\n",
    "else:\n",
    "    X3_1 = np.array(dAAOneHot(files_wp[0]))\n",
    "    X3_2 = np.array(dAAOneHot(files_wp[1]))\n",
    "    X3 = np.vstack((X3_1, X3_2)).reshape([-1,N,20])\n",
    "    \n",
    "X = np.ndarray([m1+m2,N,20,3])\n",
    "X[:,:,:,0] = X1\n",
    "X[:,:,:,1] = X2\n",
    "X[:,:,:,2] = X3\n",
    "\n",
    "y = np.zeros(m1+m2)\n",
    "y[m1:] = 1\n",
    "y = to_categorical(y,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jack-knife 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = jackknife_test(X,y,20,20,2,3,30)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = cross_validate(X,y,n_splits=10,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><strong>预测结果：</strong></font>\n",
    " - n_splits=10,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30,alexnet:<br>\n",
    " accuracy=0.8815, AUC=0.9325,mcc=0.6993<br>\n",
    " - n_splits=10,imgrow=50,imgcol=20,num_classes=2,channels=3,n_epoch=30:<br>\n",
    " accuracy=0.8846, AUC=0.9428, mcc=0.7079<br> \n",
    " - n_splits=100,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30,alexnet:<br>\n",
    " accuracy=0.889768574908648, AUC=0.9450779211871362,mcc=0.7182219182285516<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用自己构建的数据集训练和测试\n",
    "<p></p>\n",
    "<font size=4>包含800个AMP和800个非AMP</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 构建训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonfiles_wp = ['./data/benchmark/AMPs_50_hmm_profil.json','./data/benchmark/notAMPs_50_hmm_profil.json']\n",
    "files_wp=['./data/benchmark/AMPs_50.fasta','./data/benchmark/notAMPs_50.fasta']\n",
    "N = 20\n",
    "X1_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],N)\n",
    "X1_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],N)\n",
    "X1 = np.vstack((X1_1, X1_2)).reshape([-1,N,20])\n",
    "\n",
    "X2_1 = load_hmm_prof(jsonfiles_wp[0],files_wp[0],-N)\n",
    "X2_2 = load_hmm_prof(jsonfiles_wp[1],files_wp[1],-N)\n",
    "X2 = np.vstack((X2_1, X2_2)).reshape([-1,N,20])\n",
    "\n",
    "X3_1 = np.array(dAAOneHot(files_wp[0]))\n",
    "X3_2 = np.array(dAAOneHot(files_wp[1]))\n",
    "X3 = np.vstack((X3_1, X3_2)).reshape([-1,20,20])\n",
    "\n",
    "m1 = X1_1.shape[0]\n",
    "m2 = X1_2.shape[0]\n",
    "\n",
    "X = np.ndarray([m1+m2,20,20,3])\n",
    "X[:,:,:,0] = X1\n",
    "X[:,:,:,1] = X2\n",
    "X[:,:,:,2] = X3\n",
    "\n",
    "y = np.zeros(m1+m2)\n",
    "y[m1:] = 1\n",
    "y = to_categorical(y,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jack-knife测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = jackknife_test(X,y,2,3,35)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二层预测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入多标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = ['antiB', 'antiC', 'antiF', 'antiH', 'antiP', 'antiV',\n",
    "            'antiA', 'antiD', 'antiE', 'antiI', 'antiO', 'antiS', 'antiT', 'antiW', 'surface', 'taxis']\n",
    "#include_T = ['antiB', 'antiC', 'antiF', 'antiH', 'antiP', 'antiV']\n",
    "#exclude_T = ['antiA', 'antiD', 'anitE', 'antiI', 'antiO', 'antiS', 'antiT', 'antiW', 'surface', 'taxis' ]\n",
    "targetFile = './data/benchmark/amps_60_Targets.json'\n",
    "seqFile = './data/benchmark/amps_60_Sequence.json'\n",
    "include_fastafile = './data/benchmark/include_amps_60.fasta'\n",
    "exclude_fastafile = './data/benchmark/exclude_amps_60.fasta'\n",
    "\n",
    "seq_recorders = SeqIO.parse(include_fastafile, 'fasta')\n",
    "seq_ids = [str(r.id) for r in seq_recorders]\n",
    "seq_recorders.close()\n",
    "\n",
    "M = len(seq_ids)\n",
    "\n",
    "# 构建标签数据。共分6类，是include_T中包含的标签。\n",
    "# 构建一个6-by-6的矩阵，\n",
    "ft = open(targetFile,'r')\n",
    "targets = json.load(ft)\n",
    "ft.close()\n",
    "\n",
    "y = np.zeros([M,6])\n",
    "j = 0\n",
    "for key in seq_ids:\n",
    "    keys = targets[key]\n",
    "    for k in keys:\n",
    "        i = families.index(k)\n",
    "        if i < 6:\n",
    "            y[j][i] = 1\n",
    "    j+=1\n",
    "\n",
    "X1 = load_hmm_prof('./data/benchmark/includeAMPs_hmm_profil.json',include_fastafile,20)\n",
    "X2 = load_hmm_prof('./data/benchmark/includeAMPs_hmm_profil.json',include_fastafile,-20)\n",
    "X3 = np.array(dAAOneHot(include_fastafile))\n",
    "\n",
    "X = np.ndarray([X1.shape[0],20,20,3])\n",
    "X[:,:,:,0] = X1.reshape([-1,20,20])\n",
    "X[:,:,:,1] = X2.reshape([-1,20,20])\n",
    "X[:,:,:,2] = X3.reshape([-1,20,20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jack-knife测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "X_train,X_test=X[:1100],X[1100:]\n",
    "y_train,y_test=y[:1100],y[1100:]\n",
    "N = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)\n",
    "predprob = cross_validate(X,y,n_splits=10,imgrow=20,imgcol=20,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y,predprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用sigmoid函数实现多标记分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    XX = tf.placeholder(\"float\",[None, 20, 20, 3])\n",
    "    YY = tf.placeholder(\"float\",[None,6])\n",
    "    \n",
    "    # Using TFLearn wrappers for network building\n",
    "    network = conv_2d(XX, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,10,10,96]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,5,5,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,3,3,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    net = fully_connected(network, 6, activation='sigmoid')\n",
    "       \n",
    "   # loss =tf.reduce_mean( tf.nn.weighted_cross_entropy_with_logits(logits=net, targets=YY,pos_weight=0.2))\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits( logits=net, labels=YY), axis=1))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss) \n",
    "    \n",
    "    # 计算准确度\n",
    "    threshold = tf.constant(0.5,shape=y_test.shape)\n",
    "    pred_label = tf.greater(net,threshold)\n",
    "    pred_label = tf.cast(pred_label,tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred_label, YY),tf.float32))\n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    shape = tf.shape(YY)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        sess.run(shape, feed_dict={YY:y_train})\n",
    "        batch_size = 64\n",
    "        \n",
    "        for epoch in range(20):\n",
    "            n_batch = int(N/batch_size)\n",
    "            for batch in range(n_batch):\n",
    "                if (batch+1) * batch_size > N:\n",
    "                    batch_xs, batch_ys = X_train[batch*batch_size:], y_train[batch*batch_size:]\n",
    "                else:\n",
    "                    batch_xs, batch_ys = X_train[batch*batch_size:(batch+1)*batch_size], y_train[batch*batch_size:(batch+1)*batch_size]    \n",
    "                sess.run(optimizer, feed_dict={XX: batch_xs, YY: batch_ys})\n",
    "    \n",
    "            loss_val = sess.run(loss, feed_dict={XX: X_test, YY: y_test})\n",
    "            print(\"Iter \" + str(epoch) + \"Testing loss=\" + str(loss_val))\n",
    "        print(sess.run(pred_label,feed_dict={XX: X_test, YY: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    network = input_data(shape=[None, 20, 20, 3])\n",
    "    \n",
    "    # Using TFLearn wrappers for network building\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,10,10,96]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,5,5,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,3,3,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 6, activation='sigmoid')\n",
    "    network =  regression(network, optimizer='AdaGrad',\n",
    "                         loss='binary_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    model = tflearn.DNN(network)\n",
    "    model.fit(X_train, y_train, 3, shuffle=True, \n",
    "              validation_set=(X_test,y_test),\n",
    "              show_metric=True, batch_size=32)\n",
    "    pred_prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_label(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用多任务联合实现多标记分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_one_hot(y):\n",
    "    row, col = y.shape\n",
    "    y_ = np.ones(shape=(row,2*col),dtype=np.float32)\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            if y[i,j] == 1:\n",
    "                y_[i,j*2], y_[i, j*2+1] = 1, 0\n",
    "            else:\n",
    "                y_[i,j*2], y_[i, j*2+1] = 0, 1\n",
    "    return y_\n",
    "\n",
    "def decode_one_hot(y):\n",
    "    row,col = y.shape\n",
    "    y_ = np.zeros(shape=(row, col//2), dtype=np.float32)\n",
    "    for i in range(row):\n",
    "        for j in range(col//2):\n",
    "            if  y[i, j*2]  >  y[i,j*2+1]:\n",
    "                y_[i,j] = 1\n",
    "            else:\n",
    "                y_[i,j] = 0\n",
    "    return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0Testing loss=0.6705808\n",
      "Iter 1Testing loss=0.64663523\n",
      "Iter 2Testing loss=0.61493266\n",
      "Iter 3Testing loss=0.5713058\n",
      "Iter 4Testing loss=0.52238166\n",
      "Iter 5Testing loss=0.48740447\n",
      "Iter 6Testing loss=0.46974164\n",
      "Iter 7Testing loss=0.4613757\n",
      "Iter 8Testing loss=0.45710385\n",
      "Iter 9Testing loss=0.45470065\n",
      "Iter 10Testing loss=0.45323026\n",
      "Iter 11Testing loss=0.4522687\n",
      "Iter 12Testing loss=0.45160556\n",
      "Iter 13Testing loss=0.45112762\n",
      "Iter 14Testing loss=0.45076975\n",
      "Iter 15Testing loss=0.4504932\n",
      "Iter 16Testing loss=0.45027384\n",
      "Iter 17Testing loss=0.45009595\n",
      "Iter 18Testing loss=0.44994912\n",
      "Iter 19Testing loss=0.4498262\n",
      "Iter 20Testing loss=0.449722\n",
      "Iter 21Testing loss=0.44963259\n",
      "Iter 22Testing loss=0.44955522\n",
      "Iter 23Testing loss=0.4494879\n",
      "Iter 24Testing loss=0.44942874\n",
      "Iter 25Testing loss=0.4493764\n",
      "Iter 26Testing loss=0.44932985\n",
      "Iter 27Testing loss=0.44928822\n",
      "Iter 28Testing loss=0.44925088\n",
      "Iter 29Testing loss=0.44921708\n",
      "Iter 30Testing loss=0.4491865\n",
      "Iter 31Testing loss=0.44915867\n",
      "Iter 32Testing loss=0.44913325\n",
      "Iter 33Testing loss=0.44911\n",
      "Iter 34Testing loss=0.44908854\n",
      "Iter 35Testing loss=0.44906884\n",
      "Iter 36Testing loss=0.44905055\n",
      "Iter 37Testing loss=0.44903356\n",
      "Iter 38Testing loss=0.44901782\n",
      "Iter 39Testing loss=0.4490031\n",
      "Iter 40Testing loss=0.44898933\n",
      "Iter 41Testing loss=0.44897652\n",
      "Iter 42Testing loss=0.4489646\n",
      "Iter 43Testing loss=0.4489534\n",
      "Iter 44Testing loss=0.44894278\n",
      "Iter 45Testing loss=0.4489329\n",
      "Iter 46Testing loss=0.44892356\n",
      "Iter 47Testing loss=0.4489147\n",
      "Iter 48Testing loss=0.44890636\n",
      "Iter 49Testing loss=0.44889832\n",
      "Iter 0Testing loss=0.67397225\n",
      "Iter 1Testing loss=0.65402365\n",
      "Iter 2Testing loss=0.6290644\n",
      "Iter 3Testing loss=0.5947869\n",
      "Iter 4Testing loss=0.5499483\n",
      "Iter 5Testing loss=0.5069314\n",
      "Iter 6Testing loss=0.4801302\n",
      "Iter 7Testing loss=0.46683717\n",
      "Iter 8Testing loss=0.46025348\n",
      "Iter 9Testing loss=0.4567287\n",
      "Iter 10Testing loss=0.45467743\n",
      "Iter 11Testing loss=0.45339417\n",
      "Iter 12Testing loss=0.45254236\n",
      "Iter 13Testing loss=0.45194662\n",
      "Iter 14Testing loss=0.45151088\n",
      "Iter 15Testing loss=0.45117968\n",
      "Iter 16Testing loss=0.4509197\n",
      "Iter 17Testing loss=0.45071062\n",
      "Iter 18Testing loss=0.4505391\n",
      "Iter 19Testing loss=0.45039618\n",
      "Iter 20Testing loss=0.4502755\n",
      "Iter 21Testing loss=0.45017242\n",
      "Iter 22Testing loss=0.45008346\n",
      "Iter 23Testing loss=0.45000598\n",
      "Iter 24Testing loss=0.44993806\n",
      "Iter 25Testing loss=0.44987807\n",
      "Iter 26Testing loss=0.44982475\n",
      "Iter 27Testing loss=0.44977725\n",
      "Iter 28Testing loss=0.44973454\n",
      "Iter 29Testing loss=0.44969606\n",
      "Iter 30Testing loss=0.44966114\n",
      "Iter 31Testing loss=0.44962943\n",
      "Iter 32Testing loss=0.44960052\n",
      "Iter 33Testing loss=0.44957393\n",
      "Iter 34Testing loss=0.44954926\n",
      "Iter 35Testing loss=0.4495266\n",
      "Iter 36Testing loss=0.44950557\n",
      "Iter 37Testing loss=0.44948614\n",
      "Iter 38Testing loss=0.449468\n",
      "Iter 39Testing loss=0.44945103\n",
      "Iter 40Testing loss=0.44943523\n",
      "Iter 41Testing loss=0.4494204\n",
      "Iter 42Testing loss=0.44940647\n",
      "Iter 43Testing loss=0.44939336\n",
      "Iter 44Testing loss=0.44938093\n",
      "Iter 45Testing loss=0.44936928\n",
      "Iter 46Testing loss=0.4493583\n",
      "Iter 47Testing loss=0.4493479\n",
      "Iter 48Testing loss=0.44933796\n",
      "Iter 49Testing loss=0.44932863\n",
      "Iter 0Testing loss=0.6737942\n",
      "Iter 1Testing loss=0.6523975\n",
      "Iter 2Testing loss=0.6245025\n",
      "Iter 3Testing loss=0.58506167\n",
      "Iter 4Testing loss=0.5357002\n",
      "Iter 5Testing loss=0.4949448\n",
      "Iter 6Testing loss=0.47311196\n",
      "Iter 7Testing loss=0.4629445\n",
      "Iter 8Testing loss=0.45792922\n",
      "Iter 9Testing loss=0.45519865\n",
      "Iter 10Testing loss=0.45357567\n",
      "Iter 11Testing loss=0.45253938\n",
      "Iter 12Testing loss=0.45183775\n",
      "Iter 13Testing loss=0.45133862\n",
      "Iter 14Testing loss=0.45096767\n",
      "Iter 15Testing loss=0.45068225\n",
      "Iter 16Testing loss=0.4504559\n",
      "Iter 17Testing loss=0.4502725\n",
      "Iter 18Testing loss=0.45012113\n",
      "Iter 19Testing loss=0.4499943\n",
      "Iter 20Testing loss=0.4498868\n",
      "Iter 21Testing loss=0.44979477\n",
      "Iter 22Testing loss=0.44971526\n",
      "Iter 23Testing loss=0.44964594\n",
      "Iter 24Testing loss=0.44958505\n",
      "Iter 25Testing loss=0.44953108\n",
      "Iter 26Testing loss=0.44948316\n",
      "Iter 27Testing loss=0.44944024\n",
      "Iter 28Testing loss=0.44940168\n",
      "Iter 29Testing loss=0.44936687\n",
      "Iter 30Testing loss=0.44933528\n",
      "Iter 31Testing loss=0.4493065\n",
      "Iter 32Testing loss=0.44928014\n",
      "Iter 33Testing loss=0.4492559\n",
      "Iter 34Testing loss=0.44923362\n",
      "Iter 35Testing loss=0.4492131\n",
      "Iter 36Testing loss=0.44919413\n",
      "Iter 37Testing loss=0.44917652\n",
      "Iter 38Testing loss=0.44916007\n",
      "Iter 39Testing loss=0.44914478\n",
      "Iter 40Testing loss=0.44913048\n",
      "Iter 41Testing loss=0.44911703\n",
      "Iter 42Testing loss=0.44910452\n",
      "Iter 43Testing loss=0.4490927\n",
      "Iter 44Testing loss=0.44908154\n",
      "Iter 45Testing loss=0.44907114\n",
      "Iter 46Testing loss=0.44906124\n",
      "Iter 47Testing loss=0.44905195\n",
      "Iter 48Testing loss=0.44904304\n",
      "Iter 49Testing loss=0.44903454\n",
      "Iter 0Testing loss=0.67408746\n",
      "Iter 1Testing loss=0.64625263\n",
      "Iter 2Testing loss=0.6084645\n",
      "Iter 3Testing loss=0.5566822\n",
      "Iter 4Testing loss=0.5068795\n",
      "Iter 5Testing loss=0.47851944\n",
      "Iter 6Testing loss=0.46581125\n",
      "Iter 7Testing loss=0.45987034\n",
      "Iter 8Testing loss=0.45675385\n",
      "Iter 9Testing loss=0.45493442\n",
      "Iter 10Testing loss=0.45378014\n",
      "Iter 11Testing loss=0.4529992\n",
      "Iter 12Testing loss=0.45244333\n",
      "Iter 13Testing loss=0.452031\n",
      "Iter 14Testing loss=0.45171484\n",
      "Iter 15Testing loss=0.45146582\n",
      "Iter 16Testing loss=0.4512652\n",
      "Iter 17Testing loss=0.45110077\n",
      "Iter 18Testing loss=0.4509639\n",
      "Iter 19Testing loss=0.45084846\n",
      "Iter 20Testing loss=0.4507501\n",
      "Iter 21Testing loss=0.45066544\n",
      "Iter 22Testing loss=0.4505918\n",
      "Iter 23Testing loss=0.45052737\n",
      "Iter 24Testing loss=0.4504705\n",
      "Iter 25Testing loss=0.4504199\n",
      "Iter 26Testing loss=0.4503747\n",
      "Iter 27Testing loss=0.45033413\n",
      "Iter 28Testing loss=0.4502976\n",
      "Iter 29Testing loss=0.4502644\n",
      "Iter 30Testing loss=0.45023435\n",
      "Iter 31Testing loss=0.45020682\n",
      "Iter 32Testing loss=0.45018157\n",
      "Iter 33Testing loss=0.45015836\n",
      "Iter 34Testing loss=0.45013702\n",
      "Iter 35Testing loss=0.45011732\n",
      "Iter 36Testing loss=0.450099\n",
      "Iter 37Testing loss=0.45008206\n",
      "Iter 38Testing loss=0.4500662\n",
      "Iter 39Testing loss=0.45005143\n",
      "Iter 40Testing loss=0.45003757\n",
      "Iter 41Testing loss=0.45002457\n",
      "Iter 42Testing loss=0.45001233\n",
      "Iter 43Testing loss=0.45000088\n",
      "Iter 44Testing loss=0.44999003\n",
      "Iter 45Testing loss=0.44997987\n",
      "Iter 46Testing loss=0.4499703\n",
      "Iter 47Testing loss=0.4499612\n",
      "Iter 48Testing loss=0.44995266\n",
      "Iter 49Testing loss=0.44994456\n",
      "Iter 0Testing loss=0.67490053\n",
      "Iter 1Testing loss=0.6503788\n",
      "Iter 2Testing loss=0.618591\n",
      "Iter 3Testing loss=0.5732652\n",
      "Iter 4Testing loss=0.5175826\n",
      "Iter 5Testing loss=0.47433528\n",
      "Iter 6Testing loss=0.4523502\n",
      "Iter 7Testing loss=0.442266\n",
      "Iter 8Testing loss=0.4372539\n",
      "Iter 9Testing loss=0.43446824\n",
      "Iter 10Testing loss=0.43276238\n",
      "Iter 11Testing loss=0.4316371\n",
      "Iter 12Testing loss=0.43085217\n",
      "Iter 13Testing loss=0.4302804\n",
      "Iter 14Testing loss=0.42984915\n",
      "Iter 15Testing loss=0.4295146\n",
      "Iter 16Testing loss=0.4292489\n",
      "Iter 17Testing loss=0.42903364\n",
      "Iter 18Testing loss=0.42885643\n",
      "Iter 19Testing loss=0.42870826\n",
      "Iter 20Testing loss=0.4285829\n",
      "Iter 21Testing loss=0.4284758\n",
      "Iter 22Testing loss=0.4283832\n",
      "Iter 23Testing loss=0.42830262\n",
      "Iter 24Testing loss=0.4282319\n",
      "Iter 25Testing loss=0.42816934\n",
      "Iter 26Testing loss=0.4281137\n",
      "Iter 27Testing loss=0.42806396\n",
      "Iter 28Testing loss=0.42801917\n",
      "Iter 29Testing loss=0.42797884\n",
      "Iter 30Testing loss=0.42794225\n",
      "Iter 31Testing loss=0.42790884\n",
      "Iter 32Testing loss=0.42787844\n",
      "Iter 33Testing loss=0.42785054\n",
      "Iter 34Testing loss=0.4278249\n",
      "Iter 35Testing loss=0.42780122\n",
      "Iter 36Testing loss=0.4277793\n",
      "Iter 37Testing loss=0.42775893\n",
      "Iter 38Testing loss=0.42774004\n",
      "Iter 39Testing loss=0.4277224\n",
      "Iter 40Testing loss=0.42770588\n",
      "Iter 41Testing loss=0.42769036\n",
      "Iter 42Testing loss=0.4276758\n",
      "Iter 43Testing loss=0.42766213\n",
      "Iter 44Testing loss=0.42764926\n",
      "Iter 45Testing loss=0.42763698\n",
      "Iter 46Testing loss=0.4276255\n",
      "Iter 47Testing loss=0.42761457\n",
      "Iter 48Testing loss=0.42760426\n",
      "Iter 49Testing loss=0.42759448\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits)\n",
    "M = X.shape[0]\n",
    "pred_prob = np.zeros([M,6])\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    tf.reset_default_graph()\n",
    "    y_train_one_hot = encode_one_hot(y_train)\n",
    "    N = X_train.shape[0]\n",
    "    \n",
    "    XX = tf.placeholder(\"float\",[None, 20, 20, 3])\n",
    "    Y1 = tf.placeholder(\"float\",[None,2])\n",
    "    Y2 = tf.placeholder(\"float\",[None,2])\n",
    "    Y3 = tf.placeholder(\"float\",[None,2])\n",
    "    Y4 = tf.placeholder(\"float\",[None,2])\n",
    "    Y5 = tf.placeholder(\"float\",[None,2])\n",
    "    Y6 = tf.placeholder(\"float\",[None,2])\n",
    "    # Using TFLearn wrappers for network building\n",
    "    network = conv_2d(XX, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,10,10,96]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,5,5,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)#[-1,3,3,256]\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    \n",
    "    network1 = fully_connected(network, 2, activation='softmax')\n",
    "    network2 = fully_connected(network, 2, activation='softmax')\n",
    "    network3 = fully_connected(network, 2, activation='softmax')\n",
    "    network4 = fully_connected(network, 2, activation='softmax')\n",
    "    network5 = fully_connected(network, 2, activation='softmax')\n",
    "    network6 = fully_connected(network, 2, activation='softmax')\n",
    "    \n",
    "    loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network1, labels=Y1))\n",
    "    loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network2, labels=Y2))\n",
    "    loss3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network3, labels=Y3))\n",
    "    loss4 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network4, labels=Y4))\n",
    "    loss5 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network5, labels=Y5))\n",
    "    loss6 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network6, labels=Y6))\n",
    "    \n",
    "    loss = (loss1 + loss2 + loss3 + loss4 + loss5 + loss6)/6\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "    \n",
    "    pred_label = tf.concat([network1,network2,network3,network4,network5,network6],1)\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        batch_size = 64\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            n_batch = int(N/batch_size)\n",
    "            for batch in range(n_batch):\n",
    "                if (batch+1) * batch_size > N:\n",
    "                    batch_xs, batch_ys = X_train[batch*batch_size:], y_train_one_hot[batch*batch_size:]\n",
    "                else:\n",
    "                    batch_xs, batch_ys = X_train[batch*batch_size:(batch+1)*batch_size], y_train_one_hot[batch*batch_size:(batch+1)*batch_size]    \n",
    "               \n",
    "                sess.run(optimizer, \n",
    "                         feed_dict={XX: batch_xs, \n",
    "                                    Y1: batch_ys[:,0:2], \n",
    "                                    Y2: batch_ys[:,2:4],\n",
    "                                    Y3: batch_ys[:,4:6],\n",
    "                                    Y4: batch_ys[:,6:8], \n",
    "                                    Y5: batch_ys[:,8:10],\n",
    "                                    Y6: batch_ys[:,10:12]\n",
    "                                   })\n",
    "    \n",
    "            loss_val = sess.run(loss,\n",
    "                                feed_dict={XX: batch_xs, \n",
    "                                    Y1: batch_ys[:,0:2], \n",
    "                                    Y2: batch_ys[:,2:4],\n",
    "                                    Y3: batch_ys[:,4:6],\n",
    "                                    Y4: batch_ys[:,6:8], \n",
    "                                    Y5: batch_ys[:,8:10],\n",
    "                                    Y6: batch_ys[:,10:12]\n",
    "                                   })\n",
    "            print(\"Iter \" + str(epoch) + \"Testing loss=\" + str(loss_val))\n",
    "        y_pred = (sess.run(pred_label,feed_dict={XX: X_test}))\n",
    "        pred_prob[test_index] = decode_one_hot(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9937087e-01 6.2914012e-04 7.5414119e-04 9.9924588e-01 2.8470713e-01\n",
      " 7.1529281e-01 5.6186365e-04 9.9943811e-01 5.5915024e-04 9.9944085e-01\n",
      " 6.9490325e-04 9.9930513e-01]\n",
      "1.0000000232248567\n"
     ]
    }
   ],
   "source": [
    "t=y_pred[0,:]\n",
    "print(t)\n",
    "print(sum(t[2:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_one_hot(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仅hmmer profil一个通道数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1,y = load_hmm_prof()\n",
    "X1 = X1.reshape([-1,50,20,1])\n",
    "y = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    y[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    y[i][1] = 1\n",
    "# 交叉验证\n",
    "X1,y = shuffle(X1,y)\n",
    "predprob = cross_validate(X1,y,n_splits=5,num_classes=2,channels=1,n_epoch=50)\n",
    "metric(y, predprob)# print: 0.795, 0.868684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两个通道数据的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "X,y = getTwoChannelsArray()\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1\n",
    "# 交叉验证\n",
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=2,n_epoch=50)\n",
    "metric(y, predprob) # print: 0.81375, 0.883389\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> <b>预测结果</b></font><br>\n",
    "5-fold, cifanet,30 epochs, 879amps+2405 not amps, acc=0.8562, auc=0.9045, mcc=0.6470<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三个通道数据训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = getThreeChannelsArray()\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1\n",
    "# 交叉验证\n",
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=3,n_epoch=30)\n",
    "metric(y, predprob)# accuracy=0.820625 AUC=0.9052437499999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_prob = np.zeros(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test=X[:1280],X[1280:]\n",
    "y_train,y_test=y[:1280],y[1280:]\n",
    "tf.reset_default_graph()\n",
    "net = create_vggnet(2,3)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, n_epoch=30, shuffle=True, \n",
    "      show_metric=True, batch_size=32)\n",
    "predval = model.predict(X_test)\n",
    "metric(y_test, predval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predval = model.predict(X_test)\n",
    "metric(y_test, predval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>预测结果</b></font><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两联体-前20hmmrof-后20hmmprof,三个通道数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建数据集\n",
    "X1,y1 = load_hmm_prof(20,0)\n",
    "X2,y2 = load_hmm_prof(20,-1)\n",
    "X3,y3 = dAAOneHot()\n",
    "X=np.ndarray([M,20,20,3])\n",
    "X11 = X1.reshape([M,20,20])\n",
    "X21 = X2.reshape([M,20,20])\n",
    "X31 = X3.reshape([M,20,20])\n",
    "X[:,:,:,0]=X11\n",
    "X[:,:,:,1]=X21\n",
    "X[:,:,:,2]=X31\n",
    "yy = np.zeros((M,2))\n",
    "for i in range(M1):\n",
    "    yy[i][0] = 1\n",
    "for i in range(M1,M):\n",
    "    yy[i][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,yy)\n",
    "predprob = cross_validate(X,y,n_splits=5,num_classes=2,channels=3,n_epoch=40)\n",
    "metric(y, predprob)# 王普的数据，用alexnet, acc=0.90956 auc=0.957706  mcc=0.765609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仅测试代码用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "net = create_cifarnet(2,3)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, 1, shuffle=True, \n",
    "      validation_set=(X_test,y_test),\n",
    "      show_metric=True, batch_size=32)\n",
    "pred_prob= model.predict(X_test)\n",
    "metric(y_test, pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    print(\"NO. {} function has {} samples, {}%\".format(i,sum(y[:,i]), 100*sum(y[:,i])/1378))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.zeros([10,5,4])\n",
    "x1=np.ones([10,2,4])\n",
    "x[:,:2,:]=x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
