{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallerVGGNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes, finalAct=\"softmax\"):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        # CONV => RELU => POOL\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\",\n",
    "            input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # (CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # (CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # use a *softmax* activation for single-label classification\n",
    "        # and *sigmoid* activation for multi-label classification\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(finalAct))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('AMPs-ML.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([835.,  71., 381.,  46.,  55.,  75.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88., 835.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y1 = y_train[:,0]\n",
    "\n",
    "y1_cate = to_categorical(y1,num_classes=2)\n",
    "sum(y1_cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network for label 0\n",
      "[INFO] training network for label 1\n",
      "[INFO] training network for label 2\n",
      "[INFO] training network for label 3\n",
      "[INFO] training network for label 4\n",
      "[INFO] training network for label 5\n",
      "the subset accuracy: 0.23296703296703297\n",
      "hamming loss: 0.22527472527472528\n"
     ]
    }
   ],
   "source": [
    "# 分成6个2分类预测器，每个预测器预测一个标签\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "INIT_LR = 1e-3\n",
    "EPOCHS = 75\n",
    "BS = 32\n",
    "y_pred=np.zeros([y_test.shape[0],6])\n",
    "for i in range(6):\n",
    "    y_sub = y_train[:,i]\n",
    "    y_bin = to_categorical(y_sub, num_classes=2)\n",
    "    y_count = sum(y_bin)\n",
    "    cw = {0: 1, 1: y_count[0]/y_count[1]}\n",
    "    model = SmallerVGGNet.build(width=20, height=20, depth=3, classes=2, finalAct='softmax')\n",
    "    opt = Adam(lr=INIT_LR, decay=INIT_LR/EPOCHS)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    print('[INFO] training network for label {}'.format(i))\n",
    "    model.fit(X_train,y_bin, batch_size=BS, epochs=50,\n",
    "             class_weight=cw, verbose=0)\n",
    "    y_pred[:,i] = model.predict_classes(X_test)\n",
    "\n",
    "\n",
    "print(\"the subset accuracy: {}\".format(accuracy_score(y_test,pred_y)))\n",
    "print(\"hamming loss: {}\".format(hamming_loss(y_test,pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not interpret loss_weights argument: temporal - expected a list of dicts.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-12873b0ed160>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# multi-label classification, but keep in mind that the goal here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# is to treat each output label as an independent Bernoulli distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"binary_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'temporal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# train the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m             raise TypeError('Could not interpret loss_weights argument: ' +\n\u001b[0;32m    184\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                             ' - expected a list of dicts.')\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;31m# Prepare targets of model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not interpret loss_weights argument: temporal - expected a list of dicts."
     ]
    }
   ],
   "source": [
    "#同时预测6个标签\n",
    "import numpy as np\n",
    "data = np.load('AMPs-ML.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "INIT_LR = 1e-3\n",
    "EPOCHS = 75\n",
    "BS = 32\n",
    "model = SmallerVGGNet.build(width=20, height=20, depth=3, classes=6, finalAct=\"sigmoid\")\n",
    "\n",
    "# initiallize the optimizer\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR/EPOCHS)\n",
    "w = sum(y_train)\n",
    "cw = {}\n",
    "for i in range(6):\n",
    "    cw[i] = 1/(6*w[i])\n",
    "# compile the model using binary cross-entropy rather than\n",
    "# categorical cross-entropy -- this may seem counterintuitive for\n",
    "# multi-label classification, but keep in mind that the goal here\n",
    "# is to treat each output label as an independent Bernoulli distribution\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=64,\n",
    "         class_weight=cw,\n",
    "         verbose=0)\n",
    "score = model.evaluate(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(X_test)\n",
    "#print(\"the subset accuracy: {}\".format(accuracy_score(y_test,p)))\n",
    "#print(\"hamming loss: {}\".format(hamming_loss(y_test,p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7493219 , 0.1263401 , 0.44567987, 0.02329659, 0.2384791 ,\n",
       "        0.02897108],\n",
       "       [0.96842223, 0.16179664, 0.79776293, 0.00361525, 0.00816484,\n",
       "        0.00788055],\n",
       "       [0.5126319 , 0.06976548, 0.25602633, 0.09573615, 0.01815731,\n",
       "        0.1367685 ],\n",
       "       ...,\n",
       "       [0.70000565, 0.05129785, 0.42408258, 0.23366028, 0.00528744,\n",
       "        0.22436349],\n",
       "       [0.79697466, 0.03638187, 0.3964932 , 0.00966177, 0.01304887,\n",
       "        0.02583938],\n",
       "       [0.95373166, 0.17511277, 0.27926627, 0.0185288 , 0.06994769,\n",
       "        0.02416448]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
